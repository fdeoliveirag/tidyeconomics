[
  {
    "path": "posts/2021-04-16-transformacao-dos-dados/",
    "title": "Transforma√ß√£o dos dados",
    "description": "Observa√ß√µes at√≠picas, sazonalidade e tend√™ncia.",
    "author": [
      {
        "name": "Felipe O. Gon√ßalves",
        "url": {}
      }
    ],
    "date": "2021-04-16",
    "categories": [],
    "contents": "\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\n\n\n\nstn_list <- read_rds(\"stn_list.rds\")\n\nnames(stn_list)\n\n\n [1] \"rec_admin_cofins\"   \"rec_admin_csll\"     \"rec_admin_ii\"      \n [4] \"rec_admin_iof\"      \"rec_admin_ipi\"      \"rec_admin_ir\"      \n [7] \"rec_admin_pispasep\" \"rec_admin\"          \"rec_previd\"        \n[10] \"rec_total\"         \n\n\n\nstn_list %>%\n  map(\n    ~ ggplot(.x, aes(date, real)) +\n      ggseas::stat_seas(frequency = 12, start = c(1998, 02))\n  )\n\n\n$rec_admin_cofins\n\n\n$rec_admin_csll\n\n\n$rec_admin_ii\n\n\n$rec_admin_iof\n\n\n$rec_admin_ipi\n\n\n$rec_admin_ir\n\n\n$rec_admin_pispasep\n\n\n$rec_admin\n\n\n$rec_previd\n\n\n$rec_total\n\n\n\n\n\n",
    "preview": "posts/2021-04-16-transformacao-dos-dados/transformacao-dos-dados_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-04-16T17:12:41-03:00",
    "input_file": "transformacao-dos-dados.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-15-more-advanced-matching-and-manipulation/",
    "title": "More advanced matching and manipulation",
    "description": "Third chapter of \"String Manipulation with stringr in R\".",
    "author": [
      {
        "name": "Felipe O. Gon√ßalves",
        "url": {}
      }
    ],
    "date": "2021-04-15",
    "categories": [],
    "contents": "\n\n\nlibrary(stringr)\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-15T14:08:44-03:00",
    "input_file": "more-advanced-matching-and-manipulation.utf8.md"
  },
  {
    "path": "posts/2021-04-14-string-manipulation-with-stringr/",
    "title": "String manipulation with stringr",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Felipe O. Gon√ßalves",
        "url": {}
      }
    ],
    "date": "2021-04-14",
    "categories": [],
    "contents": "\n1. STRING BASICS\n1.1 Welcome!\nQUOTES\n\n\n\nline1 <- \"The table was a large one, but the three were all crowded together at one corner of it:\"\nline2 <- '\"No room! No room!\" they cried out when they saw Alice coming.'\nline3 <- \"\\\"There's plenty of room!\\\" said Alice indignantly, and she sat down in a large arm-chair at one end of the table.\"\n\n\n\n\nWHAT YOU SEE ISN‚ÄôT ALWAYS WHAT YOU HAVE\n\n\n\n(lines <- c(line1, line2, line3))\n\n\n[1] \"The table was a large one, but the three were all crowded together at one corner of it:\"                           \n[2] \"\\\"No room! No room!\\\" they cried out when they saw Alice coming.\"                                                  \n[3] \"\\\"There's plenty of room!\\\" said Alice indignantly, and she sat down in a large arm-chair at one end of the table.\"\n\n\n\n\n\nwriteLines(lines, sep = \" \")  # similar to cat()\n\n\nThe table was a large one, but the three were all crowded together at one corner of it: \"No room! No room!\" they cried out when they saw Alice coming. \"There's plenty of room!\" said Alice indignantly, and she sat down in a large arm-chair at one end of the table. \n\n\n\n\nwriteLines(\"hello\\n\\U1F30D\")  # two escaping sequences\n\n\nhello\nüåç\n\nESCAPE SEQUENCES\n\n\n# escape backslash\nwriteLines(\"To have a \\\\ you need \\\\\\\\\")\n\n\nTo have a \\ you need \\\\\n\n# escape new line\nwriteLines(\"This is a really\\nreally really\\nlong string\")\n\n\nThis is a really\nreally really\nlong string\n\n# \"Hello World\" in hindi\nhello_world <- c(\n  \"\\u0928\\u092e\\u0938\\u094d\\u0924\\u0947\",\n  \"\\u0926\\u0941\\u0928\\u093f\\u092f\\u093e\"\n)\n\ncat(hello_world)\n\n\n‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ\n\n\n\n# more info\n?Quotes\n\n\n\n1.2 Turning numbers into strings\nUSING format() WITH NUMBERS\n\n\npercent_change <- c(4, -1.91, 3.00, -5.002)\nincome         <- c(72.19, 1030.18, 10291.93, 1189192.18)\np_values       <- c(0.12, 0.98, 0.0000191, 0.00000000002)\n\n\n\n\n\n# one place after the decimal point\nformat(percent_change, digits = 2)\n\n\n[1] \" 4.0\" \"-1.9\" \" 3.0\" \"-5.0\"\n\n# whole numbers\nformat(income, digits = 2)  # scientific = FALSE, digits = 0\n\n\n[1] \"     72\" \"   1030\" \"  10292\" \"1189192\"\n\n# fixed representation\nformat(p_values, scientific = FALSE)\n\n\n[1] \"0.12000000000\" \"0.98000000000\" \"0.00001910000\" \"0.00000000002\"\n\nCONTROLLING OTHER ASPECTS OF THE STRING\n\n\nformatted_income <- format(income, digits = 2)\ntrimmed_income   <- format(income, digits = 2, trim = TRUE)\npretty_income    <- format(income, digits = 2, big.mark = \",\")\n\nwriteLines(formatted_income)\n\n\n     72\n   1030\n  10292\n1189192\n\nwriteLines(trimmed_income)\n\n\n72\n1030\n10292\n1189192\n\nwriteLines(pretty_income)\n\n\n       72\n    1,030\n   10,292\n1,189,192\n\nformatC()\n\n\n# argument 'digits' in formatC():\n## scientific format: n. of signif. digits (< 0.0001)\n## fixed format:      n. of digits after the dec. point\n\nx <- c(0.0011, 0.011, 1)\n\nwriteLines(\n  c(format(x, digits = 1),\n    \"#####\",\n    formatC(x, format = \"f\", digits = 1))\n)\n\n\n0.001\n0.011\n1.000\n#####\n0.0\n0.0\n1.0\n\n\n\nformatC(percent_change, format = \"f\", digits = 1, flag = \"+\")\n\n\n[1] \"+4.0\" \"-1.9\" \"+3.0\" \"-5.0\"\n\nformatC(p_values, format = \"g\", digits = 2)\n\n\n[1] \"0.12\"    \"0.98\"    \"1.9e-05\" \"2e-11\"  \n\n1.3 Putting strings together\nANNOTATION OF NUMBERS\n\n\n# recycled to the length of the longest vector\n(place <- paste(c(\"Here\", \"There\", \"Everywhere\"), \"a\"))\n\n\n[1] \"Here a\"       \"There a\"      \"Everywhere a\"\n\nanimal_goes <- \"moo\"\n\npaste(place, animal_goes, collapse = \", \")\n\n\n[1] \"Here a moo, There a moo, Everywhere a moo\"\n\n\n\npercent_x <- formatC(percent_change, format = \"f\", digits = 1, flag = \"+\")\n\nyears_x   <- 2010:2013\n\n\n\n\n\nyear_percent <- paste(\n  paste0(years_x, \":\"),\n  paste0(percent_x, \"%\")\n)\n\npaste(year_percent, collapse = \", \")\n\n\n[1] \"2010: +4.0%, 2011: -1.9%, 2012: +3.0%, 2013: -5.0%\"\n\nA VERY SIMPLE TABLE\n\n\nincome_names    <- c(\"Year 0\", \"Year 1\", \"Year 2\", \"Project Lifetime\")\nformatted_names <- format(income_names, justify = \"right\")\n\nfull_income   <- format(income, digits = 2, big.mark = \",\", trim = TRUE)\ndollar_income <- format(paste0(\"$\", full_income), justify = \"right\")\n\nrows <- paste(formatted_names, dollar_income, sep = \"   \")\n\nwriteLines(rows)\n\n\n          Year 0          $72\n          Year 1       $1,030\n          Year 2      $10,292\nProject Lifetime   $1,189,192\n\nLET‚ÄôS ORDER PIZZA!\n\n\ntoppings <- c(\n  \"anchovies\",       \"artichoke\",      \"bacon\",\n  \"breakfast bacon\", \"Canadian bacon\", \"cheese\",\n  \"chicken\",         \"chili peppers\",  \"feta\",\n  \"garlic\",          \"green peppers\",  \"grilled onions\",\n  \"ground beef\",     \"ham\",            \"hot sauce\",\n  \"meatballs\",       \"mushrooms\",      \"olives\",\n  \"onions\",          \"pepperoni\",      \"pineapple\",\n  \"sausage\",         \"spinach\",        \"sun-dried tomato\",\n  \"tomatoes\"\n)\n\nmy_order <- function() {\n  my_toppings     <- sample(toppings, size = 3)\n  my_toppings_and <- paste0(c(\"\", \"\", \"and \"), my_toppings)\n  these_toppings  <- paste(my_toppings_and, collapse = \", \")\n  paste0(\n    \"I want to order a pizza with \",\n    these_toppings,\n    \".\"\n  )\n}\n\nwriteLines(my_order())\n\n\nI want to order a pizza with tomatoes, ham, and chili peppers.\n\n2. INTRODUCTION TO STRINGR\n2.1 Introducing\nPUTTING STRINGS TOGETHER\n\n\nlibrary(stringr)\n\n\n\n\n\nmy_toppings     <- c(\"cheese\", NA, NA)\n\n# str_c() handles NA's different than paste0()\nmy_toppings_and <- paste0(c(\"\", \"\", \"and \"), my_toppings)\nmy_toppings_str <- str_c(c(\"\", \"\", \"and \"), my_toppings)\n\npaste(my_toppings_and, collapse = \", \")\n\n\n[1] \"cheese, NA, and NA\"\n\nstr_c(my_toppings_str, collapse = \", \")\n\n\n[1] NA\n\nSTRING LENGTH\n\n\nlibrary(babynames)\n\nbabynames_2014 <- subset(babynames, year == 2014)\n\nboys_names  <- subset(babynames_2014, sex == \"M\")$name\ngirls_names <- subset(babynames_2014, sex == \"F\")$name\n\n\n\n\n\nboys_lengths  <- str_length(boys_names)\ngirls_lengths <- str_length(girls_names)\n\nhead(data.frame(boys_names, boys_lengths))\n\n\n  boys_names boys_lengths\n1       Noah            4\n2       Liam            4\n3      Mason            5\n4      Jacob            5\n5    William            7\n6      Ethan            5\n\n# str_length() handles factors different than nchar()\nhead(str_length(factor(girls_names)))\n\n\n[1] 4 6 6 8 3 3\n\n\n\n# naive mean (not taking the proportion of appearances)\nmean(girls_lengths) - mean(boys_lengths)\n\n\n[1] 0.3374758\n\nEXTRACTING SUBSTRINGS\n\n\nboys_first_letter  <- str_sub(boys_names, 1, 1)\ntable(boys_first_letter)\n\n\nboys_first_letter\n   A    B    C    D    E    F    G    H    I    J    K    L    M    N \n1454  651  770  998  549  185  334  403  235 1390 1291  537  914  424 \n   O    P    Q    R    S    T    U    V    W    X    Y    Z \n 207  230   56  778  806  771   43  160  174   56  252  379 \n\n# str_sub() handles negative indexes different than substring()\ngirls_last_letter  <- str_sub(boys_names, -1, -1)\ntable(girls_last_letter)\n\n\ngirls_last_letter\n   a    b    c    d    e    f    g    h    i    j    k    l    m    n \n 421  104   92  436 1148   66   82  583  705   57  349  945  389 4672 \n   o    p    q    r    s    t    u    v    w    x    y    z \n 730   32   19 1011  826  292   81   71   34   86  697  119 \n\n2.2 Hunting for matches\nDETECTING MATCHES\n\n\ncontains_zz <- str_detect(boys_names, \"zz\")\n\nsum(contains_zz)\n\n\n[1] 16\n\nboys_names[contains_zz]\n\n\n [1] \"Uzziah\"    \"Ozzie\"     \"Ozzy\"      \"Jazz\"      \"Uzziel\"   \n [6] \"Chazz\"     \"Izzy\"      \"Azzam\"     \"Izzac\"     \"Izzak\"    \n[11] \"Fabrizzio\" \"Jazziel\"   \"Azzan\"     \"Izzaiah\"   \"Muizz\"    \n[16] \"Yazziel\"  \n\n\n\nsubset(babynames_2014, sex == \"M\")[contains_zz,]\n\n\n# A tibble: 16 x 5\n    year sex   name          n       prop\n   <dbl> <chr> <chr>     <int>      <dbl>\n 1  2014 M     Uzziah       67 0.0000328 \n 2  2014 M     Ozzie        62 0.0000303 \n 3  2014 M     Ozzy         57 0.0000279 \n 4  2014 M     Jazz         21 0.0000103 \n 5  2014 M     Uzziel       21 0.0000103 \n 6  2014 M     Chazz        17 0.00000832\n 7  2014 M     Izzy         16 0.00000783\n 8  2014 M     Azzam        14 0.00000685\n 9  2014 M     Izzac        13 0.00000636\n10  2014 M     Izzak         8 0.00000391\n11  2014 M     Fabrizzio     7 0.00000342\n12  2014 M     Jazziel       6 0.00000293\n13  2014 M     Azzan         5 0.00000245\n14  2014 M     Izzaiah       5 0.00000245\n15  2014 M     Muizz         5 0.00000245\n16  2014 M     Yazziel       5 0.00000245\n\nSUBSETTING STRINGS BASED ON MATCH\n\n\nstr_subset(boys_names, \"zz\")\n\n\n [1] \"Uzziah\"    \"Ozzie\"     \"Ozzy\"      \"Jazz\"      \"Uzziel\"   \n [6] \"Chazz\"     \"Izzy\"      \"Azzam\"     \"Izzac\"     \"Izzak\"    \n[11] \"Fabrizzio\" \"Jazziel\"   \"Azzan\"     \"Izzaiah\"   \"Muizz\"    \n[16] \"Yazziel\"  \n\nstr_subset(girls_names, \"zz\")\n\n\n [1] \"Izzabella\"  \"Jazzlyn\"    \"Jazzlynn\"   \"Lizzie\"     \"Izzy\"      \n [6] \"Lizzy\"      \"Mazzy\"      \"Izzabelle\"  \"Jazzmine\"   \"Jazzmyn\"   \n[11] \"Jazzelle\"   \"Jazzmin\"    \"Izzah\"      \"Jazzalyn\"   \"Jazzmyne\"  \n[16] \"Izzabell\"   \"Jazz\"       \"Mazzie\"     \"Alyzza\"     \"Izza\"      \n[21] \"Izzie\"      \"Jazzlene\"   \"Lizzeth\"    \"Jazzalynn\"  \"Jazzy\"     \n[26] \"Alizzon\"    \"Elizzabeth\" \"Jazzilyn\"   \"Jazzlynne\"  \"Jizzelle\"  \n[31] \"Izzabel\"    \"Izzabellah\" \"Izzibella\"  \"Jazzabella\" \"Jazzabelle\"\n[36] \"Jazzel\"     \"Jazzie\"     \"Jazzlin\"    \"Jazzlyne\"   \"Aizza\"     \n[41] \"Brizza\"     \"Ezzah\"      \"Fizza\"      \"Izzybella\"  \"Rozzlyn\"   \n\n\n\nstarts_U <- str_subset(girls_names, \"U\")\n\nstarts_U\n\n\n [1] \"Unique\"  \"Uma\"     \"Unknown\" \"Una\"     \"Uriah\"   \"Ursula\" \n [7] \"Unity\"   \"Umaiza\"  \"Urvi\"    \"Ulyana\"  \"Ula\"     \"Udy\"    \n[13] \"Urwa\"    \"Ulani\"   \"Umaima\"  \"Umme\"    \"Ugochi\"  \"Ulyssa\" \n[19] \"Umika\"   \"Uriyah\"  \"Ubah\"    \"Umaira\"  \"Umi\"     \"Ume\"    \n[25] \"Urenna\"  \"Uriel\"   \"Urijah\"  \"Uyen\"   \n\nstr_subset(starts_U, \"z\")\n\n\n[1] \"Umaiza\"\n\nCOUNTING MATCHES\n\n\nnumber_as <- str_count(girls_names, \"a\")\nnumber_As <- str_count(girls_names, \"A\")\n\npar(mfrow = c(1, 2))\n\nhist(number_as)\nhist(number_As)\n\n\n\n\n\n\ntotal_as <- number_as + number_As\n\ngirls_names[total_as > 4]\n\n\n[1] \"Aaradhana\"\n\n2.3 Splitting strings\nPARSING STRINGS INTO VARIABLES\n\n\ndate_ranges <- c(\n  \"23.01.2017 - 29.01.2017\",\n  \"30.01.2017 - 06.02.2017\"\n)\n\n(split_dates   <- str_split(date_ranges, fixed(\" - \")))\n\n\n[[1]]\n[1] \"23.01.2017\" \"29.01.2017\"\n\n[[2]]\n[1] \"30.01.2017\" \"06.02.2017\"\n\n(split_dates_n <- str_split(date_ranges, fixed(\" - \"), simplify = TRUE, n = 2))\n\n\n     [,1]         [,2]        \n[1,] \"23.01.2017\" \"29.01.2017\"\n[2,] \"30.01.2017\" \"06.02.2017\"\n\n\n\nstart_dates <- split_dates_n[,1]\n\nstr_split(start_dates, fixed(\".\"), simplify = TRUE)\n\n\n     [,1] [,2] [,3]  \n[1,] \"23\" \"01\" \"2017\"\n[2,] \"30\" \"01\" \"2017\"\n\n\n\nboth_names <- c(\"Box, George\", \"Cox, David\")\n\nboth_names_split <- str_split(both_names, fixed(\", \"), simplify = TRUE)\n\n(first_names <- both_names_split[,2])\n\n\n[1] \"George\" \"David\" \n\n(last_names  <- both_names_split[,1])\n\n\n[1] \"Box\" \"Cox\"\n\nSOME SIMPLE TEXT STATISTICS\n\n\n# naive line splitting\nwords <- str_split(lines, \" \")\n\nlapply(words, length)\n\n\n[[1]]\n[1] 18\n\n[[2]]\n[1] 12\n\n[[3]]\n[1] 21\n\n\n\n(word_lengths <- lapply(words, str_length))\n\n\n[[1]]\n [1] 3 5 3 1 5 4 3 3 5 4 3 7 8 2 3 6 2 3\n\n[[2]]\n [1] 3 5 2 6 4 5 3 4 4 3 5 7\n\n[[3]]\n [1]  8  6  2  6  4  5 12  3  3  3  4  2  1  5  9  2  3  3  2  3  6\n\nlapply(word_lengths, mean)\n\n\n[[1]]\n[1] 3.888889\n\n[[2]]\n[1] 4.25\n\n[[3]]\n[1] 4.380952\n\n2.4 Replacing matches in strings\nREPLACING TO TIDY STRINGS\n\n\nids <- c(\"ID#: 192\", \"ID#: 118\", \"ID#: 001\")\n\nid_nums <- str_replace(ids, \"ID#: \", \"\")\nas.numeric(id_nums)\n\n\n[1] 192 118   1\n\n\n\nphone_numbers <- c(\"510-555-0123\", \"541-555-0167\")\n\nstr_replace_all(phone_numbers, \"-\", \".\")\n\n\n[1] \"510.555.0123\" \"541.555.0167\"\n\nREVIEW\n\n\n\ngenes <- c(\n  YPO0001 = \"TTAGAGTAAATTAATCCAATCTTTGACCCAAATCTCTGCTGGATCCTCTGGTATTTCATGTTGGATGACGTCAATTTCTAATATTTCACCCAACCGTTGAGCACCTTGTGCGATCAATTGTTGATCCAGTTTTATGATTGCACCGCAGAAAGTGTCATATTCTGAGCTGCCTAAACCAACCGCCCCAAAGCGTACTTGGGATAAATCAGGCTTTTGTTGTTCGATCTGTTCTAATAATGGCTGCAAGTTATCAGGTAGATCCCCGGCACCATGAGTGGATGTCACGATTAACCACAGGCCATTCAGCGTAAGTTCGTCCAACTCTGGGCCATGAAGTATTTCTGTAGAAAACCCAGCTTCTTCTAATTTATCCGCTAAATGTTCAGCAACATATTCAGCACTACCAAGCGTACTGCCACTTATCAACGTTATGTCAGCCAT\",\n  asnC    = \"TTAAGGAACGATCGTACGCATGATAGGGTTTTGCAGTGATATTAGTGTCTCGGTTGACTGGATCTCATCAATAGTCTGGATTTTGTTGATAAGTACCTGCTGCAATGCATCAATGGATTTACACATCACTTTAATAAATATGCTGTAGTGGCCAGTGGTGTAATAGGCCTCAACCACTTCTTCTAAGCTTTCCAATTTTTTCAAGGCGGAAGGGTAATCTTTGGCACTTTTCAAGATTATGCCAATAAAGCAGCAAACGTCGTAACCCAGTTGTTTTGGGTTAACGTGTACACAAGCTGCGGTAATGATCCCTGCTTGCCGCATCTTTTCTACTCTTACATGAATAGTTCCGGGGCTAACAGCGAGGTTTTTGGCTAATTCAGCATAGGGTGTGCGTGCATTTTCCATTAATGCTTTCAGGATGCTGCGATCGAGATTATCGATCTGATAAATTTCACTCAT\",\n  asnA    = \"ATGAAAAAACAATTTATCCAAAAACAACAACAAATCAGCTTCGTAAAATCATTCTTTTCCCGCCAATTAGAGCAACAACTTGGCTTGATCGAAGTCCAGGCTCCTATTTTGAGCCGTGTGGGTGATGGAACCCAAGATAACCTTTCTGGTTCTGAGAAAGCGGTACAGGTAAAAGTTAAGTCATTGCCGGATTCAACTTTTGAAGTTGTACATTCATTAGCGAAGTGGAAACGTAAAACCTTAGGGCGTTTTGATTTTGGTGCTGACCAAGGGGTGTATACCCATATGAAAGCATTGCGCCCAGATGAAGATCGCCTGAGTGCTATTCATTCTGTATATGTAGATCAGTGGGATTGGGAACGGGTTATGGGGGACGGTGAACGTAACCTGGCTTACCTGAAATCGACTGTTAACAAGATTTATGCAGCGATTAAAGAAACTGAAGCGGCGATCAGTGCTGAGTTTGGTGTGAAGCCTTTCCTGCCGGATCATATTCAGTTTATCCACAGTGAAAGCCTGCGGGCCAGATTCCCTGATTTAGATGCTAAAGGCCGTGAACGTGCAATTGCCAAAGAGTTAGGTGCTGTCTTCCTTATAGGGATTGGTGGCAAATTGGCAGATGGTCAATCCCATGATGTTCGTGCGCCAGATTATGATGATTGGACCTCTCCGAGTGCGGAAGGTTTCTCTGGATTAAACGGCGACATTATTGTCTGGAACCCAATATTGGAAGATGCCTTTGAGATATCTTCTATGGGAATTCGTGTTGATGCCGAAGCTCTTAAGCGTCAGTTAGCCCTGACTGGCGATGAAGACCGCTTGGAACTGGAATGGCATCAATCACTGTTGCGCGGTGAAATGCCACAAACTATCGGGGGAGGTATTGGTCAGTCCCGCTTAGTGATGTTATTGCTGCAGAAACAACATATTGGTCAGGTGCAATGTGGTGTTTGGGGCCCTGAAATCAGCGAGAAAGTTGATGGCCTGCTGTAA\"\n)\n\n\n\n\n\n\n\n# number of nucleotides in each sequence\nstr_length(genes)\n\n\n[1] 441 462 993\n\n# number of A's occur in each sequence\nstr_count(genes, fixed(\"A\"))\n\n\n[1] 118 117 267\n\n# sequences containing TTTTTT\nstr_subset(genes, fixed(\"TTTTTT\"))\n\n\n[1] \"TTAAGGAACGATCGTACGCATGATAGGGTTTTGCAGTGATATTAGTGTCTCGGTTGACTGGATCTCATCAATAGTCTGGATTTTGTTGATAAGTACCTGCTGCAATGCATCAATGGATTTACACATCACTTTAATAAATATGCTGTAGTGGCCAGTGGTGTAATAGGCCTCAACCACTTCTTCTAAGCTTTCCAATTTTTTCAAGGCGGAAGGGTAATCTTTGGCACTTTTCAAGATTATGCCAATAAAGCAGCAAACGTCGTAACCCAGTTGTTTTGGGTTAACGTGTACACAAGCTGCGGTAATGATCCCTGCTTGCCGCATCTTTTCTACTCTTACATGAATAGTTCCGGGGCTAACAGCGAGGTTTTTGGCTAATTCAGCATAGGGTGTGCGTGCATTTTCCATTAATGCTTTCAGGATGCTGCGATCGAGATTATCGATCTGATAAATTTCACTCAT\"\n\n# replacing all the A's with an _ in sequences\nstr_replace_all(genes, fixed(\"A\"), \"_\")\n\n\n[1] \"TT_G_GT___TT__TCC__TCTTTG_CCC___TCTCTGCTGG_TCCTCTGGT_TTTC_TGTTGG_TG_CGTC__TTTCT__T_TTTC_CCC__CCGTTG_GC_CCTTGTGCG_TC__TTGTTG_TCC_GTTTT_TG_TTGC_CCGC_G___GTGTC_T_TTCTG_GCTGCCT___CC__CCGCCCC___GCGT_CTTGGG_T___TC_GGCTTTTGTTGTTCG_TCTGTTCT__T__TGGCTGC__GTT_TC_GGT_G_TCCCCGGC_CC_TG_GTGG_TGTC_CG_TT__CC_C_GGCC_TTC_GCGT__GTTCGTCC__CTCTGGGCC_TG__GT_TTTCTGT_G____CCC_GCTTCTTCT__TTT_TCCGCT___TGTTC_GC__C_T_TTC_GC_CT_CC__GCGT_CTGCC_CTT_TC__CGTT_TGTC_GCC_T\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n[2] \"TT__GG__CG_TCGT_CGC_TG_T_GGGTTTTGC_GTG_T_TT_GTGTCTCGGTTG_CTGG_TCTC_TC__T_GTCTGG_TTTTGTTG_T__GT_CCTGCTGC__TGC_TC__TGG_TTT_C_C_TC_CTTT__T___T_TGCTGT_GTGGCC_GTGGTGT__T_GGCCTC__CC_CTTCTTCT__GCTTTCC__TTTTTTC__GGCGG__GGGT__TCTTTGGC_CTTTTC__G_TT_TGCC__T___GC_GC___CGTCGT__CCC_GTTGTTTTGGGTT__CGTGT_C_C__GCTGCGGT__TG_TCCCTGCTTGCCGC_TCTTTTCT_CTCTT_C_TG__T_GTTCCGGGGCT__C_GCG_GGTTTTTGGCT__TTC_GC_T_GGGTGTGCGTGC_TTTTCC_TT__TGCTTTC_GG_TGCTGCG_TCG_G_TT_TCG_TCTG_T___TTTC_CTC_T\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n[3] \"_TG______C__TTT_TCC_____C__C__C___TC_GCTTCGT____TC_TTCTTTTCCCGCC__TT_G_GC__C__CTTGGCTTG_TCG__GTCC_GGCTCCT_TTTTG_GCCGTGTGGGTG_TGG__CCC__G_T__CCTTTCTGGTTCTG_G___GCGGT_C_GGT____GTT__GTC_TTGCCGG_TTC__CTTTTG__GTTGT_C_TTC_TT_GCG__GTGG___CGT____CCTT_GGGCGTTTTG_TTTTGGTGCTG_CC__GGGGTGT_T_CCC_T_TG___GC_TTGCGCCC_G_TG__G_TCGCCTG_GTGCT_TTC_TTCTGT_T_TGT_G_TC_GTGGG_TTGGG__CGGGTT_TGGGGG_CGGTG__CGT__CCTGGCTT_CCTG___TCG_CTGTT__C__G_TTT_TGC_GCG_TT___G___CTG__GCGGCG_TC_GTGCTG_GTTTGGTGTG__GCCTTTCCTGCCGG_TC_T_TTC_GTTT_TCC_C_GTG___GCCTGCGGGCC_G_TTCCCTG_TTT_G_TGCT___GGCCGTG__CGTGC__TTGCC___G_GTT_GGTGCTGTCTTCCTT_T_GGG_TTGGTGGC___TTGGC_G_TGGTC__TCCC_TG_TGTTCGTGCGCC_G_TT_TG_TG_TTGG_CCTCTCCG_GTGCGG__GGTTTCTCTGG_TT___CGGCG_C_TT_TTGTCTGG__CCC__T_TTGG__G_TGCCTTTG_G_T_TCTTCT_TGGG__TTCGTGTTG_TGCCG__GCTCTT__GCGTC_GTT_GCCCTG_CTGGCG_TG__G_CCGCTTGG__CTGG__TGGC_TC__TC_CTGTTGCGCGGTG___TGCC_C___CT_TCGGGGG_GGT_TTGGTC_GTCCCGCTT_GTG_TGTT_TTGCTGC_G___C__C_T_TTGGTC_GGTGC__TGTGGTGTTTGGGGCCCTG___TC_GCG_G___GTTG_TGGCCTGCTGT__\"\n\n\nFINAL CHALLENGES\n\n\nnames <- c(\"Diana Prince\", \"Clark Kent\")\n\nnames_split <- str_split(names, \" \", simplify = TRUE)\nabb_first   <- str_sub(names_split[,1], 1, 1)\n\nstr_c(abb_first, \". \", names_split[,2])\n\n\n[1] \"D. Prince\" \"C. Kent\"  \n\n\n\nall_names <- babynames_2014$name\n\nlast_two_letters <- str_sub(all_names, -2, -1)\nends_in_ee       <- str_detect(last_two_letters, \"ee\")\n\nsex <- babynames_2014$sex[ends_in_ee]\n\ntable(sex)\n\n\nsex\n  F   M \n572  84 \n\n\n\n\n",
    "preview": "posts/2021-04-14-string-manipulation-with-stringr/string-manipulation-with-stringr_files/figure-html5/unnamed-chunk-27-1.png",
    "last_modified": "2021-04-15T14:04:36-03:00",
    "input_file": "string-manipulation-with-stringr.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-12-comparando-investimentos-em-renda-fixa-lca-prefixado-versus-ltn/",
    "title": "Comparando Investimentos em Renda Fixa: LCA Pr√©-Fixado x LTN",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Felipe O. Gon√ßalves",
        "url": {}
      }
    ],
    "date": "2021-04-12",
    "categories": [],
    "contents": "\n\n\nlibrary(bizdays)\nlibrary(dplyr)\n\nbizdays.options$set(default.calendar = \"Brazil/ANBIMA\")\n\n\n\n\n\naliquota <- function(prazo) {\n  case_when(\n    prazo <= 0.5 ~ 22.5/100,\n    prazo <= 1   ~ 20/100,\n    prazo <= 2   ~ 17.5/100,\n    TRUE         ~ 15/100\n  )\n}\n\n\n\n\n\n# contratos\ninstr <- tibble(\n  symbol   = c(\"LTN2023\", \"LTN2026\"),\n  maturity = as.Date(c(\"2023-01-01\", \"2026-01-01\")),\n  rate     = c(5.24, 7.50)/100\n)\n\n\n\n\n\nrefdate <- Sys.Date()\nmaturity_year <- bizdays(refdate, instr$maturity)/252\n\naliquota(maturity_year)\n\n\n[1] 0.175 0.150\n\n\n\nnaked_rate <- function(rate, maturity) {\n  aliq <- aliquota(maturity)\n  nr   <- ((1 + rate)^maturity - 1)*(1 - aliq)\n  (1 + nr)^(1/maturity) - 1\n}\n\nnaked_rate(instr$rate, maturity_year)\n\n\n[1] 0.04336763 0.06494756\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-12T21:33:23-03:00",
    "input_file": "comparando-investimentos-em-renda-fixa-lca-prefixado-versus-ltn.utf8.md"
  },
  {
    "path": "posts/2021-04-12-precificando-o-ntn-f/",
    "title": "Precificando o NTN-F",
    "description": "Tesouro pr√©-fixado com juros semestrais.",
    "author": [
      {
        "name": "Felipe O. Gon√ßalves",
        "url": {}
      }
    ],
    "date": "2021-04-12",
    "categories": [],
    "contents": "\n\n\nlibrary(GetTDData)  #\nlibrary(bizdays)    #\nlibrary(scales)     #\n#\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\n#\nlibrary(ggthemes)   #\n\n\n\n\n\ntheme_set(\n  theme_solarized() +\n    theme(plot.title = element_text(face = \"bold\"),\n          axis.title = element_text(face = \"bold\"))\n)\n\n\n\n\n\nassets   <- \"NTN-F\"\nmaturity <- as.Date(\"2031-01-01\")\n\ndate_ref <- as.Date(\"2020-12-01\")\n\n\n\n\n\ndownload.TD.data(\n  asset.codes = assets,\n  dl.folder   = \"TD Files\"\n)\n\n\n\n\n\nlist.files(\"TD Files\")\n\n\n [1] \"NTN-F_2004.xls\" \"NTN-F_2005.xls\" \"NTN-F_2006.xls\"\n [4] \"NTN-F_2007.xls\" \"NTN-F_2008.xls\" \"NTN-F_2009.xls\"\n [7] \"NTN-F_2010.xls\" \"NTN-F_2011.xls\" \"NTN-F_2012.xls\"\n[10] \"NTN-F_2013.xls\" \"NTN-F_2014.xls\" \"NTN-F_2015.xls\"\n[13] \"NTN-F_2016.xls\" \"NTN-F_2017.xls\" \"NTN-F_2018.xls\"\n[16] \"NTN-F_2019.xls\" \"NTN-F_2020.xls\" \"NTN-F_2021.xls\"\n\n\n\nntnf <- read.TD.files(\n  asset.codes = assets,\n  maturity    = format(maturity, \"%d%m%y\")\n) %>%\n  as_tibble()\n\nglimpse(ntnf)\n\n\n\n\nRows: 289\nColumns: 5\n$ ref.date   <date> 2020-02-10, 2020-02-11, 2020-02-12, 2020-02-13, ‚Ä¶\n$ yield.bid  <dbl> 0.0669, 0.0657, 0.0656, 0.0664, 0.0656, 0.0654, 0‚Ä¶\n$ price.bid  <dbl> 1257.57, 1268.26, 1269.45, 1262.85, 1270.09, 1272‚Ä¶\n$ asset.code <chr> \"NTN-F 010131\", \"NTN-F 010131\", \"NTN-F 010131\", \"‚Ä¶\n$ matur.date <date> 2031-01-01, 2031-01-01, 2031-01-01, 2031-01-01, ‚Ä¶\n\n\n\nlabel_real <- partial(\n  label_dollar,\n  prefix   = \"R$\",\n  big.mark = \".\"\n)\n\nntnf %>%\n  ggplot(aes(x = ref.date, y = price.bid)) +\n  geom_line() +\n  labs(\n    x     = \"Data\",\n    y     = \"Pre√ßo\",\n    title = \"NTN-F 2031: Pre√ßos\"\n  ) +\n  scale_y_continuous(labels = label_real())\n\n\n\nntnf %>%\n  ggplot(aes(x = ref.date, y = yield.bid)) +\n  geom_line() +\n  labs(\n    x     = \"Data\",\n    y     = \"Taxa\",\n    title = \"NTN-F 2031: Taxas\"\n  ) +\n  scale_y_continuous(labels = label_percent(accuracy = 1))\n\n\n\n\n\n\nsummarize(ntnf, cor(yield.bid, price.bid))\n\n\n# A tibble: 1 x 1\n  `cor(yield.bid, price.bid)`\n                        <dbl>\n1                      -0.968\n\nApre√ßamento do t√≠tulo para uma dada taxa\n\n\n# R$ 1.000 | 10% a.a.\nparcela <- (1.1^0.5 - 1)*1000\n\ndf <- tibble(\n  date = seq(as.Date(\"2021-01-01\"), maturity,\n             by = \"6 months\")\n) %>%\n  mutate(\n    payment  = if_else(row_number() == n(),\n                       1000 + parcela,\n                       parcela),\n    adj_date = following(date,\n                         cal = \"Brazil/ANBIMA\"),\n    d2mature = bizdays(date_ref, adj_date,\n                       cal = \"Brazil/ANBIMA\")\n  )\n\nglimpse(df)\n\n\nRows: 21\nColumns: 4\n$ date     <date> 2021-01-01, 2021-07-01, 2022-01-01, 2022-07-01, 20‚Ä¶\n$ payment  <dbl> 48.80885, 48.80885, 48.80885, 48.80885, 48.80885, 4‚Ä¶\n$ adj_date <date> 2021-01-04, 2021-07-01, 2022-01-03, 2022-07-01, 20‚Ä¶\n$ d2mature <dbl> 22, 145, 273, 397, 524, 648, 773, 897, 1027, 1149, ‚Ä¶\n\n\n\ndf %>%\n  transmute(\n    yield_ref = ntnf %>%\n      filter(ref.date == date_ref) %>%\n      pull(yield.bid),\n    disc_pay  = payment / (1 + yield_ref)^(d2mature/252)\n  ) %>%\n  summarize(sum(disc_pay))\n\n\n# A tibble: 1 x 1\n  `sum(disc_pay)`\n            <dbl>\n1           1182.\n\nApre√ßamento do t√≠tulo para v√°rias taxas\n\n\nrgx <- range(ntnf$yield.bid)\nrtx <- seq(rgx[1], rgx[2], by = 0.0005)\n\nmap_dbl(\n  set_names(rtx, paste(\"t\", substring(rtx, 4), sep = \"_\")),\n  ~ df %>%\n    transmute(\n      yield_ref = .x,\n      disc_pay  = payment / (1 + yield_ref)^(d2mature/252)\n    ) %>%\n    summarize(sum(disc_pay)) %>%\n    pull()\n)\n\n\n   t_654    t_659    t_664    t_669    t_674    t_679    t_684 \n1286.280 1282.197 1278.132 1274.086 1270.058 1266.048 1262.057 \n   t_689    t_694    t_699    t_704    t_709    t_714    t_719 \n1258.083 1254.127 1250.189 1246.269 1242.366 1238.481 1234.613 \n   t_724    t_729    t_734    t_739    t_744    t_749    t_754 \n1230.762 1226.929 1223.113 1219.313 1215.531 1211.766 1208.017 \n   t_759    t_764    t_769    t_774    t_779    t_784    t_789 \n1204.286 1200.570 1196.872 1193.189 1189.524 1185.874 1182.241 \n   t_794    t_799    t_804    t_809    t_814    t_819    t_824 \n1178.623 1175.022 1171.437 1167.867 1164.313 1160.775 1157.253 \n   t_829    t_834    t_839    t_844    t_849    t_854    t_859 \n1153.746 1150.255 1146.779 1143.318 1139.872 1136.442 1133.026 \n   t_864    t_869    t_874    t_879    t_884    t_889    t_894 \n1129.626 1126.241 1122.870 1119.514 1116.173 1112.846 1109.534 \n   t_899    t_904    t_909    t_914    t_919    t_924    t_929 \n1106.236 1102.953 1099.684 1096.429 1093.189 1089.962 1086.750 \n   t_934    t_939    t_944    t_949    t_954 \n1083.551 1080.367 1077.196 1074.039 1070.895 \n\n\n\n\n",
    "preview": "posts/2021-04-12-precificando-o-ntn-f/precificando-o-ntn-f_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2021-04-13T19:14:49-03:00",
    "input_file": "precificando-o-ntn-f.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-11-estimating-portfolio-value-at-risk-var/",
    "title": "Estimating portfolio value-at-risk (VaR)",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Felipe O. Gon√ßalves",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [],
    "contents": "\nValue-at-risk and expected shortfall\nIn this chapter, you are going to measure the risk of some simple financial portfolios.\nBut what does it mean to measure the risk of a portfolio?\nConsider a fixed time period, like one day or one week, and consider the distribution of losses that would be incurred over that time period if no further trading or rebalancing of the portfolio took place.\nThis is essentially the approach that banks take when they measure the risks of their trading book. They consider the distribution of losses they could incur over two trading weeks if the trading book positions were held fixed.\nNow a risk measure is really just a statistic that describes the loss distribution. You could take the mean of the distribution or the standard deviation, but what is most common in QRM is to take statistic that describes the tail of the loss distribution and the potential for large losses.\nIn particular, it is common to take the quantile of the distribution in the tail, such as the 95th, the 99th, or the 99.5th percentile. This is the *value-at-risk concept.\nThe \\(\\alpha\\)-VaR is the alpha-quantile of the portfolio loss distribution for a specified time period. It is often called the VaR at the alpha confidence level.\nWith a probability of \\(\\alpha\\), the loss should be no more than the VaR over the time period. Of course, with a probability of \\(1-\\alpha\\), the loss could be larger and, depending on the heaviness of the tail, even quite a lot larger.\nThe VaR risk measure is used as a guide to determine how much capital a bank or other institution should have so that it can remain solvent in the face of large trading losses.\nThere is another risk measure that is commonly used in practice and which is becoming more important for determining capital requirements in banking regulation. This risk measure is known as tail VaR, conditional VaR, or expected shortfall.\nAs noted, by definition, there is a probability of \\(1-\\alpha\\) that the loss could exceed the \\(\\alpha\\)-VaR. The expected shortfall at level \\(\\alpha\\) is the expected size of the loss given that the \\(\\alpha\\)-VaR is exceeded.\nIt can be thought of as the expectation of the tail of the distribution starting at the alpha-quantile.\n\n\n# fundamental packages\nlibrary(qrmdata)\nlibrary(qrmtools)\nlibrary(xts)\n\n# other packages: QRM...\n\n\n\n\n\n# Dow Jones Index\ndata(\"DJ\")\nstr(DJ)\n\n\nAn 'xts' object on 1985-01-29/2015-12-31 containing:\n  Data: num [1:7797, 1] 1293 1288 1287 1278 1290 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr \"^DJI\"\n  Indexed by objects of class: [Date] TZ: UTC\n  xts Attributes:  \nList of 2\n $ src    : chr \"yahoo\"\n $ updated: POSIXct[1:1], format: \"2016-01-02 23:42:12\"\n\n\n\n# log-returns\ndj0809x <- diff(log(DJ))[\"2008/2009\"]\ndjx     <- as.numeric(dj0809x)\n\nplot.zoo(dj0809x, type = \"h\", col = 7)\n\n\n\n\n\n\n# 99% VaR & ES of a N(mu, sigma^2) distribution\nVaR99 <- qnorm(0.99, mean = mean(djx), sd = sd(djx))\nES99  <- QRM::ESnorm(0.99, mu = mean(djx), sd = sd(djx))\n\nround(data.frame(VaR99, ES99), 4)\n\n\n   VaR99   ES99\n1 0.0461 0.0529\n\n\n\nxvals <- seq(-4*sd(djx), 4*sd(djx), length.out = 100)\n\n# density of N(mu, sigma^2) distribution at xvals\nndens <- dnorm(xvals, mean = mean(djx), sd = sd(djx))\n\n\n\n\n\nplot(xvals, ndens, type = \"l\")\nabline(v = VaR99, col = \"red\")\nabline(v = ES99, col = \"green\")\n\n\n\n\nIn this case, ES99 is only 14.7% bigger than VaR99. For heavy-tailed distributions, the difference can be much greater.\nInternational equity portfolio\nSuppose that a UK investor has invested 30% of her wealth in the FTSE index, 40% in the SP500 index, and 30% in the SMI index. This investor is subject to five risk factors: the three indexes and the two exchange rates (GBP/USD and GBP/CHF).\nThe first thing to do is to examine the behavior of these factors:\n\n\ndata(\"FTSE\")\ndata(\"SP500\")\ndata(\"SMI\")\ndata(\"USD_GBP\")\ndata(\"CHF_GBP\")\n\nriskfactors <- merge(\n  FTSE, SP500, SMI, USD_GBP, CHF_GBP,\n  all = FALSE\n)[\"/2012\"]\n\nplot.zoo(riskfactors, col  = 2:6)\n\n\n\n\nIt is clear that there is a fall in all the major indexes around the 2008-09 crisis. The strengthtening of the Swiss Franc against the UK Pound throughout most of the period is also evident.\nIn the next exercise, you will exame the log-returns of these factors in more detail and check they conform to stylized facts. You should find as usual that they are heavy-tailed and serially dependent.\nTo estimate the portfolio loss distribution, you are going to use a simple technique called historical simulation that is widely used in the financial industry and which is based on the nonparametric idea of resampling past data.\nThe historical risk factor changes are resampled and then applied to the current portfolio in order to construct the series of losses and profits that would result if those changes happened again.\nIn order to implement historical simulation, it is necessary to write a function ‚Äì called loss operator ‚Äì that takes a vector of risk factors changes or log-returns as input and returns the resulting loss or profit on the current portfolio as output.\nThe whole history of log-returns can then be passed through this function to get the corresponding time series of historical simulated losses and gains. The distribution of these simulated data is taken as the model for the portfolio loss distribution.\nGenerally, the time series of historically simulated losses and gains also has the heavy tails and strong serial dependencies found in the underlying return series. This will be confirmed for the international equity portfolio example in one of the exercises.\nWhen the losses have been simulated, they can be used to get estimates of VaR and expected shortfall. Since VaR is a quantile, it can be estimated by taking a sample quantile of the simulated losses.\n\n\nset.seed(101)\nlosses <- rnorm(100)\n\nqlosses95 <- quantile(losses, 0.95)\n\n# sixth largest loss\nhead(sort(losses, decreasing = TRUE))\n\n\n[1] 1.852148 1.619937 1.552549 1.509897 1.427756 1.364993\n\n\n\n# VaR\ndata.frame(\n  est  = qlosses95,\n  true = qnorm(0.95)\n)\n\n\n         est     true\n95% 1.368131 1.644854\n\nSince ES is the expected loss given the loss exceeds the quantile, it can be estimated by taking the average of the simulated losses that exceed the VaR estimate.\n\n\n# ES\ndata.frame(\n  est  = mean(losses[losses > qlosses95]),\n  true = QRM::ESnorm(0.95),\n  row.names = \"95%\"\n)\n\n\n         est     true\n95% 1.592457 2.062713\n\n\n\n# log-returns\nreturns <- diff(log(riskfactors))[-1]\n\nplot.zoo(returns, col = 2:6)\n\n\n\n\n\n\n# checking for normality\n\n\n\n\n\n## Jarque-Bera test\nget_jb <- function(x) {\n  ###\n  test      <- apply(x, 2, moments::jarque.test)\n  statistic <- lapply(test, function(v) v$statistic)\n  pvalue    <- lapply(test, function(v) v$p.value)\n  ###\n  data.frame(\n    factor    = colnames(x),\n    stat      = unlist(statistic),\n    pval      = unlist(pvalue),\n    row.names = NULL\n  )\n}\n\nget_jb(returns)\n\n\n   factor     stat pval\n1  X.FTSE 4209.447    0\n2  X.GSPC 6961.586    0\n3  X.SSMI 5158.051    0\n4 USD.GBP 2586.440    0\n5 CHF.GBP 7759.171    0\n\n## Q-Q plot\nqqnorm(returns$CHF.GBP)\nqqline(returns$CHF.GBP, col = 7)\n\n\n\n\n\n\n## autocorrelation functions\n\n\n\n\n\n# from values\nacf(returns)\n\n\n\n# from absolute values\nacf(abs(returns))\n\n\n\n\nAll five risk factors are clearly non-normal and show strong serial and cross dependencies.\n\n\n# loss operator I\n# <computes the loss or gain incurred for an unitary total wealth>\n\nlossop <- function(xseries, wts = c(0.3, 0.4, 0.3)) {\n  ###\n  if (is.xts(xseries))\n    x <- coredata(xseries)\n  else if (is.matrix(xseries))\n    x <- xseries\n  else\n    x <- matrix(xseries, nrow = 1)\n  ###\n  aux <- function(x, wts) {\n    1 - (wts[1]*exp(x[1]) + wts[2]*exp(x[2]+x[4]) + wts[3]*exp(x[3]+x[5]))\n  }\n  ll <- apply(x, 1, aux, wts = wts)\n  if (is.xts(xseries))\n    ll <- xts(ll, time(xseries))\n  ###\n  ll\n}\n\n\n\n\n\n# loss from a log-return of -0.1 for all five risk factors\nlossop(rep(-0.1, 5))\n\n\n[1] 0.1554372\n\n\n\nhslosses <- lossop(returns)\n\nplot.zoo(hslosses, type = \"h\", col = 7)\n\n\n\n\n\n\nqqnorm(hslosses)\nqqline(hslosses, col = 7)\n\n\n\n\n\n\n# autocorrelation functions\npar(mfrow = c(2, 1))\n\nacf(hslosses)       # from values\nacf(abs(hslosses))  # from absolute values\n\n\n\n\nNote how the features of the underlying risk factor returns (heavy tails and serial dependence) are present in the historically simulated losses.\n\n\n# estimated values\nq99 <- quantile(hslosses, 0.99)\n\ndata.frame(\n  q  = q99,\n  es = mean(hslosses[hslosses > q99])\n)\n\n\n             q         es\n99% 0.03076173 0.04184655\n\n# true values\nmu    <- mean(hslosses)\nsigma <- sd(hslosses)\n\ndata.frame(\n  q  = qnorm(0.99, mean = mu, sd = sigma),\n  es = QRM::ESnorm(0.99, mu = mu, sd = sigma),\n  row.names = \"99%\"\n)\n\n\n             q         es\n99% 0.02602973 0.02983979\n\nThis is what we expected. The estimates derived from a normal assumption are much less conservative than the estimates derived using the nonparametric method. That is not to say that the latter method is the best possible, but is a reasonable first attempt to estimate the risk measures.\nOption portfolio and Black Scholes\nNow let‚Äôs consider investments in European options. If you buy a European call option on a stock with strike price \\(K\\) and maturity price \\(T\\) in the future, this option gives you the right but not the obligation to buy that stock at the price \\(K\\) and time \\(T\\). Obviously, if you think the stock price will rise above \\(K\\) by time \\(T\\) this could be a good buy.\nThere are also European put options. These give the right but not the obligation to sell a stock at a fixed strike and maturity. You can make money from a put if you think stock prices are going to fall.\nThe value of such an option at some time \\(t\\) depends on:\nCurrent stock price \\(S\\);\nTime until the option matures \\(T-t\\);\nRate of interest \\(r\\);\nAnnualized volatility \\(\\sigma\\) of the stock price.\nA rational method of pricing options was proposed by Black and Scholes in the 1970s based on assuming that stock prices follow the geometric Brownian motion model.\n\n\nK     <- 50\nDate  <- 2\ndate  <- 0\nS     <- 40\nr     <- 0.005\nsigma <- 0.25\n\nBlack_Scholes(\n  date,\n  S,\n  r,\n  sigma,\n  K,\n  Date,\n  \"call\"\n)\n\n\n[1] 2.619183\n\n# volatility increased by 20%\nBlack_Scholes(\n  date,\n  S,\n  r,\n  sigma*1.2,\n  K,\n  Date,\n  \"call\"\n)\n\n\n[1] 3.677901\n\nThe cost of call options increases with volatility because it enhances the chances of the stock making large gains in value during the time to maturity.\nThere is some other terminology that is used in the exercise. An option is said to be in-the-money if its current price is in the range where the option would be exercised.\nMost of the quantities required to price an option are directly observable. The maturity and strike which describe the option are known; values for the stock price and the interest rate can be easily obtained; the only parameter that poses more of a problem is volatility.\nWhile there are many methods of using historical stock prices to estimate volatility statistically, what is more common in financial markets is to use a concept called implied volatility.\nIf there is a market for options on a particular stock, then the prices that are quoted can be used to infer the implied values that would be used to price the options with the Black Scholes formula.\nTo value a portfolio containing an option, you need a value for implied volatility. Moreover, to analyze possible losses on the portfolio over some time period, you need to consider possible fluctuations of the implied volatility parameter over that time period. In other words, you need to consider implied volatility as a risk factor.\n\n\nr     <- 0.01  # interest rate\nsigma <- 0.2   # current volatility\nK     <- 100   # strike\n\nargs(Black_Scholes)\n\n\nfunction (t, S, r, sigma, K, T, type = c(\"call\", \"put\")) \nNULL\n\n\n\nbs1 <- function(price, type) {\n  Black_Scholes(\n    0, S = price, r, sigma, K, 1, type = type\n  )\n}\n\n# price a call option that matures in one year if the current stock price is 80\nbs1(80, \"call\")\n\n\n[1] 1.302245\n\n# now if the current stock price is 120\nbs1(120, \"call\")\n\n\n[1] 22.94188\n\n# price a put option that matures in one year if the current stock price is 80\nbs1(80, \"put\")\n\n\n[1] 20.30723\n\n# now if the current stock price is 120\nbs1(120, \"put\")\n\n\n[1] 1.94686\n\nDid you see how dramatically the option values changed as the stock price moved from out-of-the-money to in-the-money and vice-versa?\n\n\n# assuming a relatively constant interest rate\ndata(\"VIX\")\n\nriskfactors <- merge(SP500, VIX, all = FALSE)\n\nreturns <- diff(log(riskfactors))[-1][\"/2010\"]\n\nplot.zoo(riskfactors)\n\n\n\nplot.zoo(returns)\n\n\n\n\n\n\nplot(as.matrix(returns))\n\n\n\n\n\n\nget_jb(returns)\n\n\n  factor      stat pval\n1 X.GSPC 17397.396    0\n2  X.VIX  4412.646    0\n\nqqnorm(returns$X.VIX)\nqqline(returns$X.VIX, col = 7)\n\n\n\n\n\n\nacf(returns)\n\n\n\nacf(abs(returns))\n\n\n\n\n\n\ncor(returns)\n\n\n           X.GSPC      X.VIX\nX.GSPC  1.0000000 -0.6978308\nX.VIX  -0.6978308  1.0000000\n\nIt is clear that the log-returns of the VIX index show the same stylized facts as other returns that you have analyzed ‚Äì non-normality, heavy tails, volatility, serial dependence in absolute values but not the raw values. Moreover, they are negatively correlated with the log-returns of the SP500 index.\nHistorical simulation for the option example\nLet‚Äôs keep things really simple and consider that the investor‚Äôs portfolio contains a single European call option on an equity index (the S&P 500). Let‚Äôs also consider the loss and profit distribution over a one-day time horizon.\nWhen the option is valued using Black Scholes, then changes to the index value, the implied volatility, and the interest rate over the time horizon will all lead to changes in the value of the option.\nArguably, the first two of these risk factors are the most important and exhibit the greatest fluctuations. So let‚Äôs concentrate on changes on them and assume that the interest rate remains constant.\nTo apply the historical simulation method, a loss operator function for the option is needed. This function will take as input the log-returns of the equity index and the implied volatility and give as output the resulting loss or profit on the value of the derivative position.\nWhen you apply the loss operator function to the series, you get historically simulated losses for the derivative portfolio. Exactly as in the case of the equity investment portfolio, you can then measure the risk by estimating VaR and expected shortfall.\nRecall that a simple method of estimating VaR is to compute a sample quantile for the historically simulated data; a simple method of estimating ES is to compute the average of the losses exceeding the VaR estimate.\n\n\nlossop <- function(xseries, r = 0.01, K = 100, Time = 1, Sigma = 2, S = 100) {\n  ###\n  if (is.xts(xseries))\n    x <- coredata(xseries)\n  else if (is.matrix(xseries))\n    x <- xseries\n  else\n    x <- matrix(xseries, nrow = 1)\n  ###\n  aux <- function(x, r, K, Time, Sigma, S) {\n    deltat <- 1/250\n    V_t0   <- Black_Scholes(0, S, r, Sigma, K, Time, \"call\")\n    V_t1   <- Black_Scholes(deltat,\n                            exp(log(S) + x[1]),\n                            r,\n                            exp(log(Sigma) + x[2]),\n                            K,\n                            Time,\n                            \"call\")\n    -(V_t1-V_t0)/V_t0\n  }\n  ll <- apply(x, 1, aux, r = r, K = K, Time = Time, Sigma = Sigma, S = S)\n  if (is.xts(xseries))\n    ll <- xts(ll, time(xseries))\n  ###\n  ll\n}\n\n\n\n\n\n# first loss\nlossop(c(-0.1, -0.1), S = 80, Sigma = 0.2)\n\n\n[1] 0.8030928\n\n# second loss\nlossop(c(-0.1, 0.1), S = 100, Sigma = 0.2)\n\n\n[1] 0.4380754\n\nhslosses <- lossop(returns, S = 100, Sigma = 0.2)\n\nplot.zoo(hslosses, col = 7)\n\n\n\n\n\n\nqqnorm(hslosses)\nqqline(hslosses, col= 7)\n\n\n\n\n\n\npar(mfrow = c(2, 1))\n\nacf(hslosses)       # raw values\nacf(abs(hslosses))  # absolute values\n\n\n\n\nNote that, once again, these historically simulated losses are highly non-normal and very volatile.\n\n\nmu    <- mean(hslosses)\nsigma <- sd(hslosses)\n\ndata.frame(\n  val_at_risk = quantile(hslosses, 0.995),\n  e_shortfall = mean(hslosses[hslosses >= quantile(hslosses, 0.995)])\n)\n\n\n      val_at_risk e_shortfall\n99.5%   0.1618376   0.2211743\n\ndata.frame(\n  val_at_risk = qnorm(0.995, mean = mu, sd = sigma),\n  e_shortfall = QRM::ESnorm(0.995, mu = mu, sd = sigma),\n  row.names = \"99.5%\"\n)\n\n\n      val_at_risk e_shortfall\n99.5%   0.1441452   0.1622111\n\n\n\nlibrary(tidyverse)\nlibrary(tidyquant)\n\nhslosses_df <- tibble(\n  date   = index(hslosses),\n  losses = coredata(hslosses)\n)\n\n# expected\nhslosses_df %>%\n  summarize(\n    confidence  = \"95%\",\n    val_at_risk = quantile(losses, 0.995),\n    e_shortfall = mean(losses[losses >= val_at_risk])\n  )\n\n# true\ntibble(\n  val_at_risk = hslosses_df %>%\n      tq_performance(losses, performance_fun = VaR) %>%\n      pull(),\n  e_shortfall = hslosses_df %>%\n      tq_performance(losses, performance_fun = ES) %>%\n      pull()\n)\n\n\n\nIt will be no surprise to you now that the normal distribution greatly underestimates these measures of risk.\n\n\nreturns_w <- apply.weekly(returns, colSums)\n\nlossop <- function(xseries, r = 0.01, K = 100, Time = 1, Sigma = 2, S = 100) {\n  ###\n  if (is.xts(xseries))\n    x <- coredata(xseries)\n  else if (is.matrix(xseries))\n    x <- xseries\n  else\n    x <- matrix(xseries, nrow = 1)\n  ###\n  aux <- function(x, r, K, Time, Sigma, S) {\n    deltat <- 5/250\n    V_t0   <- Black_Scholes(0, S, r, Sigma, K, Time, \"call\")\n    V_t1   <- Black_Scholes(deltat,\n                            exp(log(S) + x[1]),\n                            r,\n                            exp(log(Sigma) + x[2]),\n                            K,\n                            Time,\n                            \"call\")\n    -(V_t1-V_t0)/V_t0\n  }\n  ll <- apply(x, 1, aux, r = r, K = K, Time = Time, Sigma = Sigma, S = S)\n  if (is.xts(xseries))\n    ll <- xts(ll, time(xseries))\n  ###\n  ll\n}\n\nhslosses_w <- lossop(returns_w, S = 120, Sigma = 0.25)\n\nquantile(hslosses_w, 0.99)\n\n\n     99% \n0.193767 \n\nConsider two things:\nThe information about serial dependence wasn‚Äôt explicitly used in the risk calculations above. It is possible to improve the risk sensitivity of the VaR and ES estimates by using some of the information about volatility clustering in the returns series. This can be achieved by a method known in industry as filtered historical simulation. The intuition is that the VaR and ES estimates are scaled by a measure of the predicted volatility for the risk horizon in question. In other words, if recent data indicate that the volatility is likely to be high, a higher scaling is applied to the VaR or ES estimate. In order to model changing volatility and make volatility predictions, more time series techniques are required, in particular ideas like GARCH models and exponentially-weighted moving average (or EWMA) volatility filters.\nThe simple empirical estimators of VaR and ES used above are not the most efficient. They are subject to large errors, particularly when the sample sizes are modest. The precision of the estimates can be improved by using parametric tail models for the historically simulated data. Relevant techniques here include heavy-tailed distributions and extreme value theory.\nSource: Quantitative Risk Management: Concepts, Techniques, and Tools.\n\n\n\n",
    "preview": "posts/2021-04-11-estimating-portfolio-value-at-risk-var/estimating-portfolio-value-at-risk-var_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-04-12T16:15:12-03:00",
    "input_file": "estimating-portfolio-value-at-risk-var.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-10-real-world-returns-are-volatile-and-correlated/",
    "title": "Real world returns are volatile and correlated",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Felipe O. Gon√ßalves",
        "url": {}
      }
    ],
    "date": "2021-04-10",
    "categories": [],
    "contents": "\nCharacteristics of volatile return series\nCan financial returns be modeled as independent and identically distributed?\nIf log-returns are iid, then the log asset price follows a random walk. However, there is a clear difference between the real log-return series and simulated iid series.\nThe real data shows a phenomenon often described as volatility clustering ‚Äì periods in which there are many large returns with differing signs interpersed with periods when the returns are more stable.\n\n\nlibrary(qrmdata)  ##\n\n# Dow Jones Index\ndata(\"DJ\")\n\n# log-returns\ndjx <- diff(log(DJ))[\"2008/2011\"]\n\n\n\n\n\nlibrary(QRM)  ##\n\n# t-Student estimate distribution parameters\ntfit  <- fit.st(djx)\ntpars <- data.frame(t(tfit$par.ests), row.names = \"^DJI\")\n\nround(tpars, 4)\n\n\n         nu     mu  sigma\n^DJI 2.4379 0.0006 0.0093\n\n\n\n# normal estimate distribution parameters\nnfit  <- fit.norm(djx)\nnpars <- data.frame(mu = nfit$mu, sigma = sqrt(nfit$Sigma[1]))\n\n\n\n\n\nn <- length(djx)\n\n\n\n\n\n# simulated data (t distribution)\nset.seed(101)\ntdata <- rt(n, df = tpars$nu) * tpars$sigma + tpars$mu\n\n\n\n\n\n# simulated data (normal distribution)\nset.seed(102)\nndata <- rnorm(n) * npars$sigma + npars$mu\n\n\n\n\n\nlibrary(xts)  ##\n\n# time series objects\ntdatax <- xts(tdata, time(djx))\nndatax <- xts(ndata, time(djx))\n\n# join\nalldata <- merge(djx, tdatax, ndatax)\n\nplot.zoo(\n  alldata,\n  type = \"h\",\n  col  = 1:3,\n  ylim = range(alldata),\n  xlab = \"Date\",\n  ylab = c(\"Index\", \"t-Student\", \"Normal\"),\n  main = toupper(\"Dow Jones Index and its simulated distributions\")\n)\n\n\n\n\nEstimating serial correlations\n\n\npar(mfrow = c(3, 1))\n\nacf(djx,   main = \"Index\")\nacf(tdata, main = \"t-Student\")\nacf(ndata, main = \"Normal\")\n\n\n\n\n\n\npar(mfrow = c(3, 1))\n\nacf(abs(djx),   main = \"Index\")\nacf(abs(tdata), main = \"t-Student\")\nacf(abs(ndata), main = \"Normal\")\n\n\n\n\n\n\npar(mfrow = c(3, 1))\n\nacf(djx^2,   main = \"Index\")\nacf(tdata^2, main = \"t-Student\")\nacf(ndata^2, main = \"Normal\")\n\n\n\n\nThe Ljung-Box test\n\\[X^2 = n(n+2)\\sum_{j=1}^k\\frac{\\hat{\\rho}(j)^2}{n-j}\\]\nNumerical test calculated from squared sample autocorrelations up to \\(k\\) lags;\nCompared with chi-squared distribution with \\(k\\) degrees of freedom;\nThe larger the value of the statistic, the greater the evidence of serial dependence.\n\n\nBox.test(djx, lag = 10, type = \"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  djx\nX-squared = 34.228, df = 10, p-value = 0.000169\n\nBox.test(abs(djx), lag = 10, type = \"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  abs(djx)\nX-squared = 1083.9, df = 10, p-value < 0.00000000000000022\n\n\n\ndata(\"DJ_const\")\n\ndjall <- diff(log(DJ_const))[\"2006/2015\"]\n\n\n\n\n\nget_ljungbox <- function(x) {\n  test <- lapply(x, Box.test, lag = 10, type = \"Ljung-Box\")\n  pval <- lapply(test, function(v) v$p.value)\n  round(unlist(pval), 4)\n}\n\n\n\n\n\nget_ljungbox(djall)  # AAPL, CAT, CSCO, HD\n\n\n  AAPL    AXP     BA    CAT   CSCO    CVX     DD    DIS     GE     GS \n0.1331 0.0000 0.0500 0.0929 0.2077 0.0000 0.0010 0.0002 0.0000 0.0001 \n    HD    IBM   INTC    JNJ    JPM     KO    MCD    MMM    MRK   MSFT \n0.1790 0.0005 0.0000 0.0000 0.0000 0.0000 0.0000 0.0014 0.0314 0.0000 \n   NKE    PFE     PG    TRV    UNH    UTX      V     VZ    WMT    XOM \n0.0000 0.0000 0.0000 0.0000 0.0000 0.0001 0.0001 0.0000 0.0000 0.0000 \n\nget_ljungbox(abs(djall))\n\n\nAAPL  AXP   BA  CAT CSCO  CVX   DD  DIS   GE   GS   HD  IBM INTC  JNJ \n   0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n JPM   KO  MCD  MMM  MRK MSFT  NKE  PFE   PG  TRV  UNH  UTX    V   VZ \n   0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n WMT  XOM \n   0    0 \n\nWhen the Ljung-Box test is applied to longer interval returns, the serial dependence gets somewhat weaker at the same time as the log returns become more normally distributed.\n\n\ndjx_m <- apply.monthly(djx, sum)\n\nBox.test(djx_m, lag = 10, type = \"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  djx_m\nX-squared = 21.099, df = 10, p-value = 0.02041\n\nBox.test(abs(djx_m), lag = 10, type = \"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  abs(djx_m)\nX-squared = 12.78, df = 10, p-value = 0.2362\n\n\n\ndjall_m <- apply.monthly(djall, colSums)\n\nget_ljungbox(djall_m)\n\n\n  AAPL    AXP     BA    CAT   CSCO    CVX     DD    DIS     GE     GS \n0.7511 0.0026 0.1312 0.0033 0.4234 0.6155 0.0774 0.0021 0.1348 0.5514 \n    HD    IBM   INTC    JNJ    JPM     KO    MCD    MMM    MRK   MSFT \n0.1057 0.6961 0.6913 0.2259 0.5656 0.0054 0.1575 0.5771 0.4118 0.7686 \n   NKE    PFE     PG    TRV    UNH    UTX      V     VZ    WMT    XOM \n0.8090 0.5855 0.7424 0.2037 0.0522 0.0174 0.4066 0.1209 0.6355 0.8258 \n\nget_ljungbox(abs(djall_m))\n\n\n  AAPL    AXP     BA    CAT   CSCO    CVX     DD    DIS     GE     GS \n0.0027 0.0000 0.0001 0.0000 0.1523 0.2888 0.0058 0.0123 0.0000 0.0016 \n    HD    IBM   INTC    JNJ    JPM     KO    MCD    MMM    MRK   MSFT \n0.2342 0.6110 0.9046 0.3454 0.0000 0.1866 0.6431 0.6790 0.0049 0.8667 \n   NKE    PFE     PG    TRV    UNH    UTX      V     VZ    WMT    XOM \n0.5800 0.0674 0.0694 0.7176 0.0000 0.0145 0.1483 0.4630 0.4772 0.1819 \n\nLooking at the extremes in volatile return series\n\n\ndata(\"FTSE\")\n\nftse_returns  <- diff(log(FTSE))[\"1991/2010\"]\n\nftse_extremes <- -ftse_returns[-ftse_returns > 0.025]\n\nlength(ftse_extremes)\n\n\n[1] 115\n\nplot(ftse_extremes, type = \"h\", auto.grid = FALSE)\n\n\n\n\nIn an iid series, the most extreme values occur completely at random, and the times between their appearance can be modeled as being very close to exponentially distributed.\n\n\ndj_returns  <- diff(log(DJ))[-1]\n\ndj_extremes <- -dj_returns[-dj_returns > 0.0274]\n\nplot(dj_extremes, type = \"h\")\n\n\n\n\n\n\ndj_spaces <- as.numeric(diff(time(dj_extremes)))\n\nhist(dj_spaces)\n\n\n\n\n\n\nlibrary(MASS)\n\nepars <- fitdistr(dj_spaces, \"exponential\")$estimate\n\nround(epars, 6)\n\n\n    rate \n0.009298 \n\nset.seed(103)\nexp_quantiles <- rexp(length(dj_spaces), rate = epars) / mean(dj_spaces)\n\nhist(exp_quantiles)\n\n\n\nqqplot(exp_quantiles, dj_spaces)\n\n\n\n\n\n\nset.seed(104)\niid_series   <- xts(rnorm(length(DJ), mean = 100), time(DJ))\n\niid_returns  <- diff(log(iid_series))\niid_extremes <- -iid_returns[-iid_returns > 0.0317]\n\n\n\n\n\nplot(iid_extremes, type = \"h\")\n\n\n\niid_spaces <- as.numeric(diff(time(iid_extremes)))\n\nhist(iid_spaces)\n\n\n\nqqplot(exp_quantiles, iid_spaces)\n\n\n\n\n\n\ndata(\"SMI\")\n\n# join Down Jones, FTSE100 and SMI\nindexes <- merge(DJ, FTSE, SMI, all = TRUE)\n\n# log-results 2005-15\nindexes <- diff(log(indexes))[\"2005/2015\"]\n\n# rows with values for the three equity indexes\nindexes <- indexes[complete.cases(indexes)]\n\n\n\n\n\nplot.zoo(\n  indexes,\n  col  = 2:4,\n  ylim = range(indexes),\n  xlab = \"Date\",\n  ylab = c(\"Dow Jones\", \"FTSE100\", \"SMI\"),\n  main = toupper(\"Daily Log Returns of Indexes\")\n)\n\n\n\n\n\n\n# correlations\n\n# matrix\ncor(indexes)\n\n\n           X.DJI    X.FTSE    X.SSMI\nX.DJI  1.0000000 0.5917692 0.5470939\nX.FTSE 0.5917692 1.0000000 0.8270541\nX.SSMI 0.5470939 0.8270541 1.0000000\n\n# plot\npairs(\n  as.zoo(indexes),\n  col         = 2:4,\n  lower.panel = NULL\n)\n\n\n\n\n\n\n# sample ACFs and cross-correlation functions\n\n# log-returns\nacf(indexes)\n\n\n\n\n\n\n# absolute log-returns\nacf(abs(indexes))\n\n\n\n\nThe stylized facts of returns series\nConsider a short-interval log-return series (daily or weekly).\nReturn series are heavier-tailed than normal (leptokurtic);\nThe volatility of return series varies over time (heteroskedastic);\nSerial correlation is relatively little in return series, while is profound in absolute returns;\nExtreme returns appear in clusters;\nReturns aggregated over longer periods tend to become more normal and less serially dependent.\n\n\ndata(\"GBP_USD\")\ndata(\"EUR_USD\")\ndata(\"JPY_USD\")\ndata(\"CHF_USD\")\n\nfx <- merge(GBP_USD, EUR_USD, 1/JPY_USD, CHF_USD, all = TRUE)\ncolnames(fx)[3] <- \"USD.JPY\"\n\nfx <- diff(log(fx))[\"2011/2015\"]\n\nfx_w <- apply.weekly(fx, colSums)\n\nplot.zoo(fx, type = \"h\", col = 2:5)\n\n\n\nplot.zoo(fx_w, type = \"h\", col = 2:5)\n\n\n\n\n\n\nacf(fx)\n\n\n\nacf(abs(fx))\n\n\n\n\n\n\nget_ljungbox(fx)\n\n\nGBP.USD EUR.USD USD.JPY CHF.USD \n      0       0       0       0 \n\nget_ljungbox(abs(fx))\n\n\nGBP.USD EUR.USD USD.JPY CHF.USD \n      0       0       0       0 \n\n\n\nacf(fx_w)\n\n\n\nacf(abs(fx_w))\n\n\n\n\n\n\nget_ljungbox(fx_w)\n\n\nGBP.USD EUR.USD USD.JPY CHF.USD \n 0.5426  0.4091  0.6491  0.3605 \n\nget_ljungbox(abs(fx_w))\n\n\nGBP.USD EUR.USD USD.JPY CHF.USD \n 0.0000  0.0012  0.0000  0.0002 \n\n\n\nacf(zcb_x)\nacf(abs(zcb_x))\n\napply(zcb_x, 2, Box.test, lag = 10, type = \"Ljung\")\napply(abs(zcb_x), 2, Box.test, lag = 10, type = \"Ljung\")\n\n# the monthly log-returns for the 5 and 10 year yields don't appear to show much serial dependence\nacf(zcbx_m)\nacf(abs(zcbx_m))\n\napply(zcbx_m, 2, Box.test, lag = 10, type = \"Ljung\")\napply(abs(zcbx_m), 2, Box.test, lag = 10, type = \"Ljung\")\n\n\n\nWhich is the true statement?\nA significant Ljung-Box test implies that the data are heavier-tailed than normal\nA insignificant Ljung-Box test implies that the time series can be assumed to be independent\nA Q-Q plot of the waiting times between the most extreme values against a normal distribution can reveal evidence of volatility [F, its against an exponential distribution]\nWhen volatility is present in a time series we can predict the magnitude of future returns\nWhen volatility is present in a time series we can predict the direction of future returns [F]\n\n\n\n",
    "preview": "posts/2021-04-10-real-world-returns-are-volatile-and-correlated/real-world-returns-are-volatile-and-correlated_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2021-04-11T18:13:47-03:00",
    "input_file": "real-world-returns-are-volatile-and-correlated.utf8.md",
    "preview_width": 1248,
    "preview_height": 1152
  },
  {
    "path": "posts/2021-03-29-explorando-o-sidra-com-tidyverse-parte-ii/",
    "title": "Explorando o SIDRA com tidyverse - Parte II",
    "description": "Usando a PNAD Cont√≠nua para ilustrar a trajet√≥ria recente do mercado de trabalho no Brasil e compar√°-la ao √∫ltimo per√≠odo recessivo.",
    "author": [
      {
        "name": "Filippe O. Gon√ßalves",
        "url": {}
      }
    ],
    "date": "2021-04-01",
    "categories": [
      "SIDRA",
      "PNADC"
    ],
    "contents": "\nAproveitando a derradeira divulga√ß√£o dos dados da PNAD Cont√≠nua, esta\npostagem se dedica √† visualiza√ß√£o de indicadores centrais para a an√°lise\ndo mercado de trabalho brasileiro.\nEm particular, compara-se o desempenho atual com o per√≠odo recessivo\nentre 2014 e 2016, tornando expl√≠cito o car√°ter sui generis da crise\natual.\nPacotes\n\n\nlibrary(sidrar)     # importa√ß√£o via API\nlibrary(ggseas)     # gr√°fico sem sazonalidade\nlibrary(ggthemes)   # est√©tica do gr√°fico\n#\nlibrary(tidyverse)\n\n\n\nDados\nO procedimento de importa√ß√£o dos dados ser√° o mesmo da primeira\npostagem da s√©rie, por√©m aplicado √† Tabela 6318 da PNADC/M.\nEscolhendo apenas a quantidade total de pessoas (em milhares) como\nvari√°vel, para todas as condi√ß√µes e trimestres m√≥veis, a disposi√ß√£o dos\nblocos fica assim:\n\nEnt√£o, obt√©m-se sua consequente express√£o:\n\n\nt6318 <- \"/t/6318/n1/all/v/1641/p/all/c629/all\"\n\n\n E a partir dela os dados originais, diretamente da fonte para o\nR:\n\n\ndf_ibge <- sidrar::get_sidra(api = t6318)\n\nnames(df_ibge)\n\n\n\n [1] \"N√≠vel Territorial (C√≥digo)\"                                             \n [2] \"N√≠vel Territorial\"                                                      \n [3] \"Unidade de Medida (C√≥digo)\"                                             \n [4] \"Unidade de Medida\"                                                      \n [5] \"Valor\"                                                                  \n [6] \"Brasil (C√≥digo)\"                                                        \n [7] \"Brasil\"                                                                 \n [8] \"Vari√°vel (C√≥digo)\"                                                      \n [9] \"Vari√°vel\"                                                               \n[10] \"Trimestre M√≥vel (C√≥digo)\"                                               \n[11] \"Trimestre M√≥vel\"                                                        \n[12] \"Condi√ß√£o em rela√ß√£o √† for√ßa de trabalho e condi√ß√£o de ocupa√ß√£o (C√≥digo)\"\n[13] \"Condi√ß√£o em rela√ß√£o √† for√ßa de trabalho e condi√ß√£o de ocupa√ß√£o\"         \n\n\nManipula√ß√£o\nBem como na Parte I, aqui tamb√©m s√£o necess√°rias apenas tr√™s\ncolunas:\n\n\npnad <- df_ibge %>%\n  as_tibble() %>%\n  select(\n    date  = \"Trimestre M√≥vel (C√≥digo)\",\n    name  = \"Condi√ß√£o em rela√ß√£o √† for√ßa de trabalho e condi√ß√£o de ocupa√ß√£o\",\n    value = \"Valor\"\n  )\n\nhead(pnad, 5)\n\n# A tibble: 5 √ó 3\n  date   name                            value\n  <chr>  <chr>                           <dbl>\n1 201203 Total                          155670\n2 201203 For√ßa de trabalho               95191\n3 201203 For√ßa de trabalho - ocupada     87632\n4 201203 For√ßa de trabalho - desocupada   7559\n5 201203 Fora da for√ßa de trabalho       60479\n\nPor√©m, o ideal para esta postagem √© modificar o padr√£o dos dados de\n‚Äúcomprido‚Äù para ‚Äúlargo‚Äù com tidyr::pivot_wider(), fazendo\ncom que cada categoria possua sua pr√≥pria coluna (e cada data torne-se\n√∫nica): \n\n\npnad <- pnad %>%\n  pivot_wider(names_from = name, values_from = value) %>%\n  rename_with(~ c(\"pia\", \"pea\", \"ocup\", \"desocup\", \"pnea\"), -date)\n\ntail(pnad)\n\n# A tibble: 6 √ó 6\n  date      pia    pea  ocup desocup  pnea\n  <chr>   <dbl>  <dbl> <dbl>   <dbl> <dbl>\n1 202008 174600  95460 81666   13794 79141\n2 202009 175121  96556 82464   14092 78565\n3 202010 175555  98361 84301   14061 77193\n4 202011 176014  99601 85578   14023 76413\n5 202012 176362 100104 86179   13925 76258\n6 202101 176674 100297 86025   14272 76377\n\n Finalmente, chega-se ao que interessa para os gr√°ficos:\n‚Äî Os trimestres m√≥veis transformados em objeto de tempo com\nreadr::parse_date(); ‚Äî A popula√ß√£o ocupada em\nmilh√µes; ‚Äî As taxas de desemprego e de participa√ß√£o na for√ßa de\ntrabalho.\n\n\npnad <- pnad %>%\n  transmute(\n    date   = parse_date(date, format = \"%Y%m\"),\n    ocup   = ocup / 1000,\n    desemp = desocup / pea * 100,\n    partic = pea / pia * 100\n  )\n\nglimpse(pnad)\n\nRows: 107\nColumns: 4\n$ date   <date> 2012-03-01, 2012-04-01, 2012-05-01, 2012-06-01, 2012‚Ä¶\n$ ocup   <dbl> 87.632, 88.407, 88.863, 89.129, 89.181, 89.428, 89.63‚Ä¶\n$ desemp <dbl> 7.940877, 7.748897, 7.607690, 7.517510, 7.433933, 7.2‚Ä¶\n$ partic <dbl> 61.14923, 61.50632, 61.68484, 61.69382, 61.62181, 61.‚Ä¶\n\n\nVisualiza√ß√£o\nOptando por um tema inspirado na The Economist para todos os\ngr√°ficos:\n\n\ntheme_set(ggthemes::theme_economist())\n\n\nNo primeiro gr√°fico, tem-se a popula√ß√£o ocupada de janeiro de 2019 a\njaneiro de 2021. Para ressaltar valores importantes, criou-se uma nova\nvari√°vel apenas com primeira, √∫ltima, m√≠nima e m√°xima do per√≠odo. Al√©m\ndisso, detalhou-se melhor a escala de tempo:\n\n\npnad %>%\n  filter(date >= \"2019-01-01\") %>%\n  mutate(\n    txt = if_else(\n      ocup %in% c(first(ocup), last(ocup), min(ocup), max(ocup)),\n      ocup, NULL\n    )\n  ) %>%\n  ggplot(aes(x = date, y = ocup, label = txt)) +\n  geom_line() +\n  geom_text() +\n  scale_x_date(date_labels = \"%b %Y\")\n\n\n\n\n\nUma realidade que d√° sentido ao segundo gr√°fico. Nele, procurou-se\nmostrar ‚Äî com ggplot2::annotate() ‚Äî a diferen√ßa entre a\ncrise atual e o √∫ltimo per√≠odo recessivo, que compreende dez trimestres\n(2014.Q3 a 2016.Q4) de acordo com o CODACE.\nA t√°tica mais usada em R para plotar m√∫ltiplas s√©ries sob um mesmo\nper√≠odo √© a combina√ß√£o entre tidyr::pivot_longer() e\nggplot2::facet_wrap(), resgatando o padr√£o ‚Äúcomprido‚Äù para\nas vari√°veis de interesse (no caso, as taxas).\nFinalmente, o pacote {ggseas} permitiu dessazonalizar as\ns√©ries ‚Äúon the fly‚Äù para plotagem. O m√©todo X-13 foi usado.\n\n\n# in√≠cio e fim dos per√≠odos recessivos\ncodace <- as.Date(c(\"2014-07-01\", \"2016-12-01\", \"2020-01-01\"))\n\npnad %>%\n  pivot_longer(c(desemp, partic)) %>%\n  ggplot(aes(x = date, y = value, color = name)) +\n  ggseas::stat_seas(geom = \"line\", frequency = 12, start = c(2012, 3)) +\n  annotate(\"rect\",\n           xmin  = codace[c(1, 3)], xmax  = c(codace[2], Inf),\n           ymin  = c(-Inf, -Inf),   ymax  = c(Inf, Inf),\n           alpha = .3) +\n  facet_wrap(~ name, ncol = 1, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n\nA oferta de trabalho, que se manteve praticamente inalterada em\n2014-16, sofreu um baque de 5 pontos percentuais durante a pandemia\n(sendo 7 at√© agosto e 2 recuperados posteriormente).\nSem essa quebra na s√©rie, o desemprego estaria num patamar\nconsideravelmente maior (mesmo partindo de uma taxa j√° elevada, de 11,2%\nem janeiro de 2020).\n###\n\n\n\n",
    "preview": "posts/2021-03-29-explorando-o-sidra-com-tidyverse-parte-ii/explorando-o-sidra-com-tidyverse-parte-ii_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2022-09-17T16:15:36-03:00",
    "input_file": "explorando-o-sidra-com-tidyverse-parte-ii.knit.md",
    "preview_width": 1248,
    "preview_height": 864
  },
  {
    "path": "posts/2021-03-12-raca-clusters-de-renda-e-classificacao-de-bairros-no-r/",
    "title": "Ra√ßa, clusters de renda e classifica√ß√£o de bairros no R",
    "description": "Um algoritmo simples de machine learning sobre a rela√ß√£o entre ra√ßa e renda nas cidades brasileiras.",
    "author": [
      {
        "name": "Filippe O. Gon√ßalves",
        "url": {}
      }
    ],
    "date": "2021-03-12",
    "categories": [
      "Censo Demogr√°fico"
    ],
    "contents": "\nEsta postagem representa um esfor√ßo preliminar no sentido de testar a\npossibilidade de prever o n√≠vel de renda de um bairro brasileiro apenas\ncom base em sua configura√ß√£o racial.\nO caso analisado ser√° a cidade de origem do autor: a antiga Jo√£o\nPessoa, que conta com 63 bairros registrados no IBGE at√© ent√£o. O\nalgoritmo, todavia, √© generaliz√°vel para qualquer cidade. Seu\ninstrumental √© dividido em duas partes:\nNa primeira (n√£o supervisionada), uma clusteriza√ß√£o\nk-means agrupa os bairros em tr√™s classes relativas de renda: alta,\nm√©dia e baixa;\nNa segunda (supervisionada), uma classifica√ß√£o log√≠stica\nn√£o-bin√°ria prev√™ a qual cluster o bairro pertence dada sua\npropor√ß√£o de indiv√≠duos brancos.\nPacotes\n\n\nlibrary(geobr)      # mapa de bairros\nlibrary(janitor)    # limpeza de nomes\nlibrary(MASS)       # logit ordenado\n#\nlibrary(tidyverse)\n\n\n\nDados\nA mat√©ria-prima da an√°lise √© a Tabela\n3177 do Censo Demogr√°fico 2010.\nAp√≥s sua importa√ß√£o, um trabalho razo√°vel de limpeza ‚Äî valores\nausentes, concis√£o, renivelamento e reordena√ß√£o de categorias ‚Äî resulta\nnum data frame que apresenta as diferentes combina√ß√µes para os 63\nbairros entre 5 grupos raciais e 8 grupos de renda. √â poss√≠vel baix√°-lo\nneste\nreposit√≥rio.\n\n\ndf_pop <- read_rds(\"censo_3177_pb.rds\")\n\n# ex. bairro mais populoso da cidade, popula√ß√£o parda\nfilter(df_pop, bairro == \"Mangabeira\", racial == \"Parda\")\n\n# A tibble: 9 √ó 4\n  bairro     racial renda              valor\n  <chr>      <fct>  <fct>              <dbl>\n1 Mangabeira Parda  At√© 1/2 SM          1656\n2 Mangabeira Parda  Mais de 1/2 a 1 SM  8345\n3 Mangabeira Parda  Mais de 1 a 2 SM    6439\n4 Mangabeira Parda  Mais de 2 a 3 SM    1738\n5 Mangabeira Parda  Mais de 3 a 5 SM    1195\n6 Mangabeira Parda  Mais de 5 a 10 SM    509\n7 Mangabeira Parda  Mais de 10 SM         68\n8 Mangabeira Parda  Sem rendimento     12430\n9 Mangabeira Parda  Total              32380\n\n\n\n\nE para garantir uma visualiza√ß√£o espacial dos dados, os pol√≠gonos dos\nbairros podem ser importados atrav√©s do excelente\n{geobr}:\n\n\nsf_bairros <- geobr::read_neighborhood(year = 2010) %>%\n  filter(code_muni == 2507507) %>%  # c√≥digo de Jo√£o Pessoa\n  select(name_neighborhood, geom)\n\n\n\nContexto\nUm panorama das combina√ß√µes ra√ßa-renda para o agregado dos bairros √©\nbastante elucidativo e, dado o escopo do blog, suficiente como\njustificativa:\n\n\ndf_pop %>%\n  # para cada combina√ß√£o entre ra√ßa e estrato de renda...\n  group_by(racial, renda) %>%\n  # ...o agregado de todos os bairros; e para cada grupo racial...\n  summarize(valor = sum(valor)) %>%\n  # ...a propor√ß√£o de cada estrato;\n  mutate(valor = valor / last(valor)) %>%\n  # formato largo (estratos de renda em colunas)\n  pivot_wider(names_from = renda, values_from = valor)\n\n\nRendimento\nmensal por grupo racial em Jo√£o Pessoa (2010)\n\n\n\n\nAs fra√ß√µes da popula√ß√£o branca ganhando de dois sal√°rios m√≠nimos em\ndiante s√£o sempre ‚Äî e gradativamente ‚Äî maiores que as respectivas\nfra√ß√µes dos outros grupos raciais. Estas, por sua vez, somente decrescem\nna medida em que a renda aumenta. \nget_split()\nUma boa t√°tica para lidar com as vari√°veis √© cindir os dados para\nelas terem suas bases pr√≥prias, manipul√°-las separadamente e depois\nreuni-las.\nEvitando repeti√ß√£o desnecess√°ria, √© poss√≠vel construir com tidy evaluation uma\nfun√ß√£o usando dplyr::enquo() para chamar as colunas dentro\ndo pipe:\n\n\nget_split <- function(y, x) {\n  # vari√°vel\n  x <- enquo(x)\n  # data frame\n  y %>%\n    # indiv√≠duos por bairro pertencendo a cada categoria da vari√°vel\n    group_by(bairro, !!x) %>%\n    summarize(valor = sum(valor), .groups = \"drop\") %>%\n    # categorias formando colunas\n    pivot_wider(names_from = !!x, values_from = valor) %>%\n    # limpando os nomes das colunas\n    janitor::clean_names()\n}\n\n\n\nPropor√ß√£o de brancos\nCom os dados raciais, precisa-se da medida quantitativa de\nbranquitude dos bairros:\n\n\nbranq <- df_pop %>%\n  get_split(racial) %>%\n  summarize(bairro,\n            prop_branq = branca / total)\n\nglimpse(branq)\n\nRows: 63\nColumns: 2\n$ bairro     <chr> \"Aeroclube\", \"√Ågua Fria\", \"Altiplano Cabo Branco\"‚Ä¶\n$ prop_branq <dbl> 0.6256013, 0.5011933, 0.5281250, 0.2883413, 0.339‚Ä¶\n\n\n\n# branquitude\nbranq %>%\n  ggplot(aes(x = prop_branq)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density()\n\n\nDistribui√ß√£o\nracial dos bairros de Jo√£o Pessoa (2010)\n\n\n\n\nClusters de renda\nNo caso dos dados de renda, tem-se como medida a participa√ß√£o de cada\nestrato na popula√ß√£o ocupada dos bairros:\n\n\nrendim <- df_pop %>%\n  get_split(renda) %>%\n  mutate(across(where(is.numeric), ~ .x / (total - sem_rendimento))) %>%\n  select(bairro, contains(\"_sm\"))\n\nglimpse(rendim)\n\nRows: 63\nColumns: 8\n$ bairro             <chr> \"Aeroclube\", \"√Ågua Fria\", \"Altiplano Cabo‚Ä¶\n$ ate_1_2_sm         <dbl> 0.008200837, 0.011596548, 0.067423231, 0.‚Ä¶\n$ mais_de_1_2_a_1_sm <dbl> 0.1358996, 0.2014563, 0.2830441, 0.598030‚Ä¶\n$ mais_de_1_a_2_sm   <dbl> 0.2060251, 0.2758900, 0.1965955, 0.200843‚Ä¶\n$ mais_de_2_a_3_sm   <dbl> 0.12702929, 0.17637540, 0.09012016, 0.031‚Ä¶\n$ mais_de_3_a_5_sm   <dbl> 0.17456067, 0.18959008, 0.11014686, 0.019‚Ä¶\n$ mais_de_5_a_10_sm  <dbl> 0.213221757, 0.121089536, 0.143858478, 0.‚Ä¶\n$ mais_de_10_sm      <dbl> 0.1350627615, 0.0240021575, 0.1088117490,‚Ä¶\n\nSeja por silhueta ou cotovelo, o n√∫mero √≥timo de clusters varia entre\ndois e tr√™s. Para trabalhar com a ideia de classe m√©dia, optou-se pelo\n√∫ltimo:\n\n\nset.seed(321)  # ordem dos clusters preservada\n\nk_rendim <- rendim %>%\n  summarize(bairro,\n            cluster = select(., -bairro) %>%\n              kmeans(centers   = 3,\n                     nstart    = 10,\n                     algorithm = \"Hartigan-Wong\") %>%\n              pluck(\"cluster\") %>%\n              as_factor())\n\nglimpse(k_rendim)\n\nRows: 63\nColumns: 2\n$ bairro  <chr> \"Aeroclube\", \"√Ågua Fria\", \"Altiplano Cabo Branco\", \"‚Ä¶\n$ cluster <fct> 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 2, 2, 2, 1, 1, 2, 1‚Ä¶\n\n\n\n# geografia\nk_rendim %>%\n  mutate(bairro = str_to_title(bairro)) %>%\n  inner_join(sf_ibge, by = c(\"bairro\" = \"name_neighborhood\")) %>%\n  ggplot(aes(geometry = geom, fill = cluster)) +\n  geom_sf()\n\n\nClusters dos\nbairros de Jo√£o Pessoa por renda (2010)\n\nNota: O ‚Äúburaco‚Äù no meio do mapa, na verdade, s√£o mais de 500\nhectares remanescentes de Mata Atl√¢ntica natural, que abrigam o Jardim\nBot√¢nico da cidade.\n\n\n\n\nClassifica√ß√£o\nEstimando por logit ordenado:\n\n\npred_logit <- k_rendim %>%\n  inner_join(branq, by = \"bairro\") %>%\n  mutate(pred = (cluster ~ prop_branq) %>%\n           MASS::polr(method = \"logistic\") %>%\n           predict(),\n         acc  = (cluster == pred))\n\nglimpse(pred_logit)\n\nRows: 63\nColumns: 5\n$ bairro     <chr> \"Aeroclube\", \"√Ågua Fria\", \"Altiplano Cabo Branco\"‚Ä¶\n$ cluster    <fct> 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 2, 2, 2, 1, 1, 2‚Ä¶\n$ prop_branq <dbl> 0.6256013, 0.5011933, 0.5281250, 0.2883413, 0.339‚Ä¶\n$ pred       <fct> 3, 2, 3, 1, 1, 3, 2, 2, 3, 3, 3, 2, 3, 1, 1, 1, 2‚Ä¶\n$ acc        <lgl> TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE‚Ä¶\n\nNa pr√°tica, o que acontece com esse modelo bivariado √© simplesmente\numa otimiza√ß√£o da escolha de tr√™s intervalos de branquitude para o\nencaixe de cada cluster:\n\n\n# intervalos\npred_logit %>%\n  filter(acc == TRUE) %>%\n  group_by(cluster) %>%\n  summarize(min_prop_branq = min(prop_branq),\n            max_prop_branq = max(prop_branq))\n\n# A tibble: 3 √ó 3\n  cluster min_prop_branq max_prop_branq\n  <fct>            <dbl>          <dbl>\n1 1                0.284          0.377\n2 2                0.396          0.501\n3 3                0.517          0.699\n\nPor√©m, a dimens√£o racial da estrutura socioecon√¥mica da cidade √© t√£o\nforte que mesmo essa simplicidade √© capaz de prever corretamente o n√≠vel\nde renda de 51 dos 63 bairros:\n\n\n# acur√°cia por grupos\npred_logit %>%\n  group_by(cluster) %>%\n  summarize(true        = sum(acc == TRUE),\n            false       = n() - true,\n            performance = true / n())\n\n# A tibble: 3 √ó 4\n  cluster  true false performance\n  <fct>   <int> <int>       <dbl>\n1 1          21     3       0.875\n2 2          12     6       0.667\n3 3          18     3       0.857\n\n# acur√°cia total\nsummarize(pred_logit, performance = sum(acc == TRUE) / n())\n\n# A tibble: 1 √ó 1\n  performance\n        <dbl>\n1       0.810\n\nLogo, ao separar os bairros de Jo√£o Pessoa em tr√™s faixas de\nbranquitude: i) de 28 a 38%; ii) de 39 a 50% e; iii) de 51 a 70% de\nbrancos, torna-se poss√≠vel classificar corretamente mais de 4/5 deles\nentre baixa, m√©dia e alta renda.\n\n\n# visualiza√ß√£o final\npred_logit %>%\n  ggplot(aes(x = prop_branq, y = cluster)) +\n  geom_boxplot(aes(fill = cluster)) +\n  geom_jitter(aes(color = acc))\n\n\nBairros de Jo√£o\nPessoa: Ra√ßa vs.¬†Renda (2010)\n\n\n\n\n\n\n\n###\n\n\n\n",
    "preview": "posts/2021-03-12-raca-clusters-de-renda-e-classificacao-de-bairros-no-r/explorando-o-sidra-com-tidyverse-parte-ii_files/figure-html5/unnamed-chunk-20-1.png",
    "last_modified": "2022-09-17T16:14:03-03:00",
    "input_file": "explorando-o-sidra-com-tidyverse-parte-ii.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-22-explorando-o-sidra-com-tidyverse-parte-i/",
    "title": "Explorando o SIDRA com tidyverse - Parte I",
    "description": "Usando a composi√ß√£o do IPCA para calcular e visualizar o padr√£o da infla√ß√£o acumulada no Brasil em tempos de Covid.",
    "author": [
      {
        "name": "Filippe O. Gon√ßalves",
        "url": {}
      }
    ],
    "date": "2021-02-22",
    "categories": [
      "SIDRA",
      "IPCA"
    ],
    "contents": "\nEste blog estreia com a primeira parte de uma sequ√™ncia de postagens\nque procurar√° explorar as bases de dados do SIDRA atrav√©s de uma\nabordagem avan√ßada de workflow no R.\nNa postagem de hoje, calcula-se a varia√ß√£o mensal acumulada do IPCA\ndesagregado em grupos para, em seguida, construir-se um gr√°fico sobre a\ninfla√ß√£o brasileira durante a pandemia ‚Äì com aten√ß√£o especial √† infla√ß√£o\nde alimentos. \nPacotes\n\n\nlibrary(sidrar)     # importa√ß√£o via API\nlibrary(ggthemes)   # est√©tica do gr√°fico\n#\nlibrary(tidyverse)\n\n\n\nDados\nN√£o ser√° preciso baixar nenhum arquivo. Seguindo os passos abaixo,\nencontra-se a express√£o que permitir√° levar os dados direto para o\nR:\nAcessar o portal do SIDRA\nIPCA > Rela√ß√£o de Tabelas > 7061\nDefinir op√ß√µes:\nVari√°vel: varia√ß√£o mensal (%)\nGeral, grupo, subgrupo, item e subitem: 9\ncomponentes\nM√™s: mar/2020 a jan/2021\nUnidade Territorial: Brasil\n\nArrastar os blocos at√© ficarem dispostos assim:\n\nMenu Inferior Direito: Links de Compartilhar > Par√¢metros para a\nAPI\n Feito. Cria-se ent√£o um objeto com a express√£o (a partir de\n\"/t\"):\n\n\n\nt7061 <- \"/t/7061/n1/all/v/306/p/last%2011/c315/7170,7445,7486,7558,7625,7660,7712,7766,7786/d/v306%202\"\n\n\n\n E esse objeto √© suficiente para importar os dados por meio do\nprovidencial {sidrar}:\n\n\ndf_ibge <- sidrar::get_sidra(api = t7061)\n\nnames(df_ibge)\n\n\n\n [1] \"N√≠vel Territorial (C√≥digo)\"                     \n [2] \"N√≠vel Territorial\"                              \n [3] \"Unidade de Medida (C√≥digo)\"                     \n [4] \"Unidade de Medida\"                              \n [5] \"Valor\"                                          \n [6] \"Brasil (C√≥digo)\"                                \n [7] \"Brasil\"                                         \n [8] \"Vari√°vel (C√≥digo)\"                              \n [9] \"Vari√°vel\"                                       \n[10] \"M√™s (C√≥digo)\"                                   \n[11] \"M√™s\"                                            \n[12] \"Geral, grupo, subgrupo, item e subitem (C√≥digo)\"\n[13] \"Geral, grupo, subgrupo, item e subitem\"         \n\n\nManipula√ß√£o\nDas treze colunas, apenas tr√™s ser√£o necess√°rias aqui:\n\n\nipca <- df_ibge %>%\n  as_tibble() %>%\n  select(\n    date          = \"M√™s\",\n    component     = \"Geral, grupo, subgrupo, item e subitem\",\n    mth_inflation = \"Valor\"\n  )\n\nhead(ipca, 9)\n\n# A tibble: 9 √ó 3\n  date       component                   mth_inflation\n  <chr>      <chr>                               <dbl>\n1 mar√ßo 2020 1.Alimenta√ß√£o e bebidas              0.84\n2 mar√ßo 2020 2.Habita√ß√£o                          0.13\n3 mar√ßo 2020 3.Artigos de resid√™ncia             -1.08\n4 mar√ßo 2020 4.Vestu√°rio                          0.21\n5 mar√ßo 2020 5.Transportes                       -0.9 \n6 mar√ßo 2020 6.Sa√∫de e cuidados pessoais          0.21\n7 mar√ßo 2020 7.Despesas pessoais                 -0.23\n8 mar√ßo 2020 8.Educa√ß√£o                           0.59\n9 mar√ßo 2020 9.Comunica√ß√£o                        0.04\n\nOs dados v√™m num padr√£o ‚Äúcomprido‚Äù ao inv√©s de ‚Äúlargo‚Äù: n√£o h√° uma\ncoluna para cada componente, e isso faz com que se tenha nove\nobserva√ß√µes para cada m√™s. Diante da finalidade desta postagem, o ideal\nser√° mant√™-los assim.\n O c√°lculo da infla√ß√£o acumulada para \\(t \\in \\{1,\\ ..., 11\\}\\) meses de pandemia √©\nrecursivo (depende dos pr√≥prios valores defasados). De uma varia√ß√£o\nmensal inicial \\(\\pi_1^{ac} = \\pi_1\\),\ntem-se\n\\[\\pi_t^{ac} = (1\\ +\\ \\frac{\\pi_t}{100})\\\n\\times\\ \\pi_{t-1}^{ac}\\ +\\ \\pi_t\\ \\]\npara \\(t\\neq1\\). Essa atribui√ß√£o\nrequer a fun√ß√£o purrr::accumulate() para ser realizada\ndentro do flow ‚Äî em parceria com dplyr::group_by() para ser\naplicada a cada componente em separado:\n\n\nipca <- ipca %>%\n  group_by(component) %>%          # agrupamento\n  mutate(\n    # .y\n    accum_inflation = accumulate(\n      mth_inflation,               # .x\n      ~ (1 + .x / 100) * .y + .x   # atribui√ß√£o recursiva de .y\n    )\n  )\n\n\n Conferindo o primeiro e mais\nrelevante componente do per√≠odo:\n\n\nhead(arrange(ipca, component), 11)\n\n# A tibble: 11 √ó 4\n# Groups:   component [1]\n   date          component               mth_inflation accum_inflation\n   <chr>         <chr>                           <dbl>           <dbl>\n 1 mar√ßo 2020    1.Alimenta√ß√£o e bebidas          0.84            0.84\n 2 abril 2020    1.Alimenta√ß√£o e bebidas          1.57            2.42\n 3 maio 2020     1.Alimenta√ß√£o e bebidas          0.33            2.76\n 4 junho 2020    1.Alimenta√ß√£o e bebidas          0.64            3.42\n 5 julho 2020    1.Alimenta√ß√£o e bebidas          0.42            3.85\n 6 agosto 2020   1.Alimenta√ß√£o e bebidas          1.17            5.07\n 7 setembro 2020 1.Alimenta√ß√£o e bebidas          2.54            7.74\n 8 outubro 2020  1.Alimenta√ß√£o e bebidas          1.9             9.78\n 9 novembro 2020 1.Alimenta√ß√£o e bebidas          2.28           12.3 \n10 dezembro 2020 1.Alimenta√ß√£o e bebidas          1.37           13.8 \n11 janeiro 2021  1.Alimenta√ß√£o e bebidas          0.68           14.6 \n\n Agora, chega-se ao valor acumulado no final do per√≠odo para\ntodos os componentes:\n\n\n# jan/2021\nipca <- summarize(ipca, accum_inflation = last(accum_inflation))\n\n\n E eis a mat√©ria-prima do gr√°fico:\n\n\narrange(ipca, desc(accum_inflation))\n\n# A tibble: 9 √ó 2\n  component                   accum_inflation\n  <chr>                                 <dbl>\n1 1.Alimenta√ß√£o e bebidas             14.6   \n2 3.Artigos de resid√™ncia              7.07  \n3 2.Habita√ß√£o                          3.98  \n4 9.Comunica√ß√£o                        3.11  \n5 6.Sa√∫de e cuidados pessoais          1.41  \n6 5.Transportes                        1.36  \n7 7.Despesas pessoais                  0.749 \n8 4.Vestu√°rio                          0.0240\n9 8.Educa√ß√£o                          -2.50  \n\nNota: Ao olhar para a defla√ß√£o dos servi√ßos educacionais, deve-se ter\nem conta que fevereiro, um m√™s particularmente\nimportante para o componente, √© o √∫nico que n√£o se faz presente nos\ndados. \nVisualiza√ß√£o\nO primeiro passo para o sucesso do gr√°fico de barras ser√°, seguindo o\nembalo da formata√ß√£o, reordenar a sequ√™ncia de componentes com\nforcats::fct_reorder() de acordo com o tamanho da infla√ß√£o\nacumulada:\n\n\nipca <- ipca %>%\n  mutate(\n    component = component %>%\n      # retirada de numera√ß√£o\n      str_sub(3) %>%\n      # concis√£o\n      recode(\"Alimenta√ß√£o e bebidas\"     = \"Alimenta√ß√£o\",\n             \"Artigos de resid√™ncia\"     = \"Resid√™ncia\",\n             \"Sa√∫de e cuidados pessoais\" = \"Sa√∫de\",\n             \"Despesas pessoais\"         = \"Desp. Pessoais\") %>%\n      # crit√©rio de ordena√ß√£o\n      fct_reorder(accum_inflation)\n  )\n\n\n A carca√ßa do gr√°fico j√° pode ser criada:\n\n\nipca %>%\n  ggplot(aes(accum_inflation, component)) +\n  geom_col()\n\n\n\nPor√©m ela √© pouco apelativa (apesar de ter seu charme). O segundo\npasso ser√° desfrutar das possibilidades de montagem em cima dessa\ncarca√ßa ‚Äî por exemplo, a fun√ß√£o ggplot2::geom_label().\nEsmiu√ß√°-las exigiria uma postagem √† parte.\nPor √∫ltimo, escolhe-se tema e paleta de cores a gosto. Como a\ninspira√ß√£o para esta postagem veio justamente de mat√©rias\njornal√≠sticas sobre a trajet√≥ria recente da infla√ß√£o de alimentos,\nfoi escolhido um padr√£o do Wall Street\nJournal encontrado no {ggthemes}:\n\n\n\nUm senhor ganho de apelo est√©tico.\nNa segunda parte da sequ√™ncia, ser√° poss√≠vel trabalhar com planilhas\nmais complexas do SIDRA. \n###\n\n\n\n",
    "preview": "posts/2021-02-22-explorando-o-sidra-com-tidyverse-parte-i/desmistificando-o-sidra-parte-i_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2022-09-17T16:11:59-03:00",
    "input_file": "desmistificando-o-sidra-parte-i.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
